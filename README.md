# ğŸ® Hangman AI Solver

<div align="center">

### Combining Hidden Markov Models + Deep Reinforcement Learning

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Success Rate](https://img.shields.io/badge/Success%20Rate-94.40%25-brightgreen.svg)](.)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](.)

A sophisticated AI agent that plays Hangman with a 94.40% success rate by combining Hidden Markov Models, Deep Reinforcement Learning, and intelligent word filtering.

</div>

---

## ğŸ‘¥ Team Members

| Name | SRN |
|:----:|:---:|
| **Mohammed Musharraf** | PES2UG23CS915 |
| **Mohammed Shehzaad Khan** | PES2UG23CS349 |
| **Mohammed Bilal** | PES2UG23CS344 |
| **Mohammed Aahil** | PES2UG23CS342 |

---

## ğŸ¯ Results

<div align="center">

| Metric | Value |
|:------:|:-----:|
| ğŸ† **Success Rate** | **94.40%** (2000 test games) |
| ğŸ“‰ **Avg Wrong Guesses** | **2.13** per game |
| ğŸ² **Repeated Guesses** | **0** |
| ğŸ”„ **Training Win Rate** (Pure RL) | 7.18% |
| âœ¨ **Testing Win Rate** (Hybrid) | **94.40%** |

</div>

---

## ğŸ—ï¸ System Architecture

### Three-Part Hybrid System

Our system uses a **three-part hybrid approach**:

### 1. ğŸ§  Hidden Markov Model (HMM)

- **States:** Position-based (0, 1, 2, ..., word_length-1)
- **Emissions:** Letters (A-Z)
- **Models:** 24 separate HMMs for word lengths 1-24
- **Smoothing:** Laplace smoothing (Î±=1.0) for unseen patterns
- **Purpose:** Captures positional letter patterns in English words

### 2. ğŸ” Word Filtering System

- Matches current pattern against corpus words
- When â‰¤20 words match, uses direct letter frequency
- **Most powerful component** of the system
- Provides highly accurate predictions for narrow search spaces

### 3. ğŸ¤– Deep Q-Network (DQN) Agent

**State Representation (619 dimensions):**
- ğŸ¯ Masked word (540 dims): 20 positions Ã— 27 one-hot features
- âœ… Guessed letters (26 dims): Binary vector
- â¤ï¸ Lives remaining (1 dim): Normalized
- ğŸ“Š HMM probabilities (26 dims)
- ğŸ“ˆ Word filter probabilities (26 dims)

**Neural Network:**
```
619 â†’ 256 â†’ 128 â†’ 64 â†’ 26
```

**Training Features:**
- Experience replay buffer: 10,000 transitions
- Target network updated every 10 episodes
- Epsilon-greedy exploration: 1.0 â†’ 0.01

### âš¡ Hybrid Strategy

```python
if matching_words <= 20:
    return word_filter_prediction()
else:
    return blend(
        word_filtering=50%,
        hmm_predictions=30%,
        dqn_q_values=20%
    )
```

---

## ğŸ“ Project Structure

```
ML-Hackathon/
â”œâ”€â”€ ğŸ“‚ Data/
â”‚   â”œâ”€â”€ corpus.txt                             # 50,000 training words
â”‚   â””â”€â”€ test.txt                               # 2,000 test words
â”œâ”€â”€ ğŸ““ ML_Hackathon_915_349_344_342.ipynb      # Main implementation notebook
â”œâ”€â”€ ğŸ“„ Analysis_Report.pdf                     # Detailed analysis report
â””â”€â”€ ğŸ“– README.md                               # This file
```

---

## ğŸš€ Quick Start

### Setup

1. **Install dependencies:**

```bash
pip install torch tqdm matplotlib numpy
```

2. **Prepare data files:**

- Place `corpus.txt` in `Data/` directory (50,000 words)
- Place `test.txt` in `Data/` directory (2,000 words)

### Running the Notebook

The notebook `ML_Hackathon_915_349_344_342.ipynb` contains three main parts:

**Part 1: HMM Training** (~1 minute)
- Loads and preprocesses corpus
- Trains 24 HMMs (one per word length)
- Creates word matcher for filtering
- Saves models: `hangman_models.pkl`, `word_matcher.pkl`

**Part 2: RL Agent Training** (~19 minutes on GPU)
- Loads trained HMMs
- Creates Hangman environment
- Trains DQN agent for 5000 episodes
- Saves model: `dqn_agent.pth`

**Part 3: Evaluation** (~2 minutes)
- Loads all models
- Creates hybrid agent
- Evaluates on 2000 test words
- Generates visualizations
- Saves results: `evaluation_results.pkl`

### Expected Output

```
ğŸ§  HANGMAN HMM TRAINING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Loaded 50000 words
Training HMMs for each word length...
âœ… HMM Training Complete!

ğŸ¤– HANGMAN RL TRAINING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000
Final Win Rate: 7.18%
âœ… Training Complete!

ğŸ“Š HANGMAN EVALUATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Evaluating Hybrid Agent: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000
Success Rate: 94.40%
âœ… Evaluation Complete!
```

---

## ğŸ“Š Key Findings

<table>
<tr>
<td width="50%">

### âœ… What Worked

- **ğŸ† Hybrid approach dominated**  
  94.40% vs 7.18% for pure RL

- **ğŸ” Word filtering was critical**  
  Direct corpus matching = strongest signal

- **ğŸ§  HMM captured patterns**  
  Position-specific letter probabilities

</td>
<td width="50%">

### âŒ What Didn't Work

- **ğŸ¤– Pure RL struggled**  
  619-dimensional state space too complex

- **âš ï¸ Sparse rewards**  
  Mostly negative feedback hindered learning

- **ğŸŒ Conservative exploration**  
  700 episodes to reach min epsilon

</td>
</tr>
</table>

---

## ğŸ¯ Reward Function

| Action | Reward |
|:-------|:------:|
| âœ… Correct guess | **+10** per position revealed |
| ğŸŠ Win game | **+100** bonus |
| âŒ Wrong guess | **-15** penalty |
| ğŸ’€ Lose game | **-100** penalty |
| ğŸ” Repeated guess | **-20** efficiency penalty |

---

## ğŸ“ˆ Performance by Word Length

<div align="center">

| Word Length | Win Rate | Context Level |
|:-----------:|:--------:|:-------------:|
| 2-4 letters | 50-80% | âš ï¸ Limited |
| 5-9 letters | **95%+** | âœ… Optimal |
| 10-15 letters | **95%+** | âœ… Many clues |
| 16+ letters | **~100%** | ğŸ¯ Extensive |

</div>

---

## ğŸ”§ Training Parameters

<div align="center">

| Parameter | Value |
|:---------:|:-----:|
| ğŸ”„ **Episodes** | 5000 |
| ğŸ“¦ **Batch Size** | 64 |
| ğŸ“š **Learning Rate** | 0.001 |
| ğŸ’° **Gamma (Discount)** | 0.95 |
| ğŸ² **Epsilon Start** | 1.0 |
| ğŸ¯ **Epsilon Min** | 0.01 |
| ğŸ“‰ **Epsilon Decay** | 0.995 |
| ğŸ’¾ **Replay Buffer** | 10,000 |
| ğŸ”„ **Target Update** | Every 10 episodes |

</div>

---

## ğŸ“ Generated Files

| File | Description |
|:-----|:------------|
| ğŸ§  `hangman_models.pkl` | Trained HMM models for all word lengths |
| ğŸ” `word_matcher.pkl` | Word filtering system |
| ğŸ¤– `dqn_agent.pth` | Trained DQN agent weights |
| ğŸ“Š `training_results.png` | Training curves visualization |
| ğŸ“ˆ `evaluation_results.pkl` | Evaluation statistics |
| ğŸ¨ `evaluation_results.png` | Evaluation visualizations |

---

## ğŸ’¡ Key Lessons Learned

<div align="center">

### ğŸŒŸ Four Core Insights

</div>

<br>

> **1. ğŸ§  Domain knowledge beats pure learning**  
> Explicit word matching outperformed neural networks

> **2. ğŸ—ï¸ System design matters**  
> Intelligent combination > any single method

> **3. âš¡ RL as refinement**  
> Use RL for edge cases, not learning entire strategy

> **4. âœ¨ Simple can be better**  
> Word filtering was simpler AND more effective

---

## ğŸ“š References

- ğŸ§  Hidden Markov Models for sequence prediction
- ğŸ¤– Deep Q-Networks (DQN) for reinforcement learning
- ğŸ’¾ Experience replay and target networks
- ğŸ² Epsilon-greedy exploration strategy

---

<div align="center">

### ğŸ“ **PES University**
**Machine Learning Hackathon 2025**

*Date: November 3, 2025*

<br>

**Made with â¤ï¸ by Team Mohammed**

</div>
