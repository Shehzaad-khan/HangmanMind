# Hangman AI Solver ğŸ®# ğŸ® Hangman AI Solver<div align="center"># Hangman AI Solver - HMM + Deep Reinforcement Learning



A sophisticated AI agent that plays Hangman with a 94.40% success rate by combining Hidden Markov Models, Deep Reinforcement Learning, and intelligent word filtering.



## Team Members> **A hybrid approach combining Hidden Markov Models, Deep Reinforcement Learning, and Word Filtering**



| Name | SRN |

|------|-----|

| Mohammed Musharraf | PES2UG23CS915 |## ğŸ‘¥ Team Members# ğŸ® Hangman AI Solver## Team Members

| Mohammed Shehzaad Khan | PES2U23CS349 |

| Mohammed Bilal | PES2UG23CS344 |

| Mohammed Aahil | PES2UG23CS342 |

| Name | SRN |### *Combining Hidden Markov Models + Deep Reinforcement Learning*

## Overview

|------|-----|

This project implements a hybrid AI system that combines three complementary approaches to achieve exceptional performance in the Hangman word-guessing game:

| Mohammed Musharraf | PES2UG23CS915 || Name | SRN |

- **Hidden Markov Models (HMM)**: Learn position-specific letter patterns from a corpus of 50,000 words

- **Deep Q-Network (DQN)**: Reinforcement learning agent that learns strategic decision-making| Mohammed Shehzaad Khan | PES2U23CS349 |

- **Word Filtering**: Pattern matching against the corpus for highly accurate predictions

| Mohammed Bilal | PES2UG23CS344 |<br>|------|-----|

## Results

| Mohammed Aahil | PES2UG23CS342 |

Our hybrid system achieved outstanding performance on 2,000 test games:

| Mohammed Musharraf | PES2UG23CS915 |

- **Success Rate**: 94.40%

- **Average Wrong Guesses**: 2.13 per game---

- **Repeated Guesses**: 0

- **Improvement over Pure RL**: 94.40% vs 7.18% (13x better)[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)| Mohammed Shehzaad Khan | PES2U23CS349 |



## Architecture## ğŸ¯ Results



### 1. Hidden Markov Model (HMM)[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)| Mohammed Bilal | PES2UG23CS344 |



The HMM component captures positional letter distributions in English words.| Metric | Value |



**Key Features:**|--------|-------|[![Success Rate](https://img.shields.io/badge/Success%20Rate-94.40%25-brightgreen.svg)](.)| Mohammed Aahil | PES2UG23CS342 |

- 24 separate models trained for word lengths 1-24

- Position-based states (0, 1, 2, ..., word_length-1)| **Success Rate** | 94.40% (2000 test games) |

- Letter emissions (A-Z)

- Laplace smoothing (Î±=1.0) for unseen patterns| **Avg Wrong Guesses** | 2.13 per game |[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](.)



**How it works:**| **Repeated Guesses** | 0 |

- Analyzes where specific letters typically appear in words of a given length

- Provides probability distributions for the next letter guess based on revealed positions| **Training Win Rate** (Pure RL) | 7.18% |---

- Example: 'E' is more likely at the end of words, 'S' at the beginning of plurals

| **Testing Win Rate** (Hybrid) | 94.40% |

### 2. Word Filtering System

</div>

The most powerful component that performs direct pattern matching.

---

**Strategy:**

- Filters corpus to match the current masked word pattern## ğŸ¯ Final Results

- When â‰¤20 words match, uses direct letter frequency from matching words

- Highly accurate for constrained search spaces## ğŸ“ Project Structure

- Example: Pattern `"a_ple"` â†’ matches "apple", "ample" â†’ suggests 'p'

---

### 3. Deep Q-Network (DQN)

```

Reinforcement learning agent that learns to combine all available signals.

ML-Hackathon/- **Success Rate:** 94.40% on 2000 test games

**State Space (619 dimensions):**

- Masked word representation (540 dims): 20 positions Ã— 27 features (26 letters + blank)â”œâ”€â”€ Data/

- Guessed letters (26 dims): Binary vector indicating which letters were tried

- Lives remaining (1 dim): Normalized count of remaining incorrect guessesâ”‚   â”œâ”€â”€ corpus.txt              # 50,000 training words## ğŸ‘¥ Team Members- **Average Wrong Guesses:** 2.13 per game

- HMM predictions (26 dims): Probability distribution from HMM

- Word filter predictions (26 dims): Probability distribution from word matchingâ”‚   â””â”€â”€ test.txt                # 2,000 test words



**Network Architecture:**â”œâ”€â”€ Untitled14.ipynb            # Main implementation notebook- **Repeated Guesses:** 0

```

Input Layer (619) â”œâ”€â”€ Analysis_Report.md          # Detailed analysis report

    â†“

Dense Layer (256) + ReLUâ””â”€â”€ README.md                   # This file<div align="center">- **Training Win Rate (Pure RL):** 7.18%

    â†“

Dense Layer (128) + ReLU```

    â†“

Dense Layer (64) + ReLU- **Testing Win Rate (Hybrid System):** 94.40%

    â†“

Output Layer (26)---

```

| Name | SRN |

**Training Details:**

- Episodes: 5,000## ğŸ—ï¸ System Architecture

- Experience replay buffer: 10,000 transitions

- Epsilon-greedy exploration: 1.0 â†’ 0.01 (decay: 0.995)|:----:|:---:|## ğŸ“ Project Structure

- Target network update: Every 10 episodes

- Optimizer: Adam (lr=0.001)Our system uses a **three-part hybrid approach**:

- Discount factor (Î³): 0.95

- Training time: ~19 minutes on GPU| **Mohammed Musharraf** | PES2UG23CS915 |



### Hybrid Decision Strategy### 1. ğŸ§  Hidden Markov Model (HMM)



The system intelligently combines all three components:- **States:** Position-based (0, 1, 2, ..., word_length-1)| **Mohammed Shehzaad Khan** | PES2U23CS349 |```



```python- **Emissions:** Letters (A-Z)

if len(matching_words) <= 20:

    # Use direct word filtering for high confidence- **Models:** 24 separate HMMs for word lengths 1-24| **Mohammed Bilal** | PES2UG23CS344 |ML-Hackathon/

    return word_filter_prediction()

else:- **Smoothing:** Laplace smoothing (Î±=1.0)

    # Blend all sources with weighted combination

    prediction = (- **Purpose:** Captures positional letter patterns in English words| **Mohammed Aahil** | PES2UG23CS342 |â”œâ”€â”€ Data/

        0.50 * word_filter_probabilities +

        0.30 * hmm_probabilities +

        0.20 * dqn_q_values

    )### 2. ğŸ” Word Filtering Systemâ”‚   â”œâ”€â”€ corpus.txt           # 50,000 training words

    return best_unguessed_letter(prediction)

```- Matches current pattern against corpus words



## Project Structure- When â‰¤20 words match â†’ uses direct letter frequency</div>â”‚   â””â”€â”€ test.txt             # 2,000 test words



```- **Most powerful component** of the system

ML-Hackathon/

â”œâ”€â”€ Data/- Provides highly accurate predictions for narrow search spacesâ”œâ”€â”€ Untitled14.ipynb         # Main implementation notebook

â”‚   â”œâ”€â”€ corpus.txt              # 50,000 training words

â”‚   â””â”€â”€ test.txt                # 2,000 test words

â”œâ”€â”€ Untitled14.ipynb            # Main implementation notebook

â”œâ”€â”€ Analysis_Report.md          # Detailed project analysis### 3. ğŸ¤– Deep Q-Network (DQN) Agent---â”œâ”€â”€ Analysis_Report.md       # Detailed analysis report

â””â”€â”€ README.md                   # This file

```



## Getting Started**State Representation (619 dimensions):**â””â”€â”€ README.md                # This file



### Prerequisites- Masked word (540 dims): 20 positions Ã— 27 one-hot features



```bash- Guessed letters (26 dims): Binary vector## ğŸ¯ Final Results```

pip install torch tqdm matplotlib numpy

```- Lives remaining (1 dim): Normalized



### Data Setup- HMM probabilities (26 dims)



Ensure you have the following files in the `Data/` directory:- Word filter probabilities (26 dims)

- `corpus.txt` - Training corpus (50,000 words)

- `test.txt` - Test set (2,000 words)<div align="center">## ğŸ—ï¸ Architecture



### Running the Notebook**Network Architecture:**



The `Untitled14.ipynb` notebook is divided into three sequential parts:```



#### Part 1: HMM Training (~1 minute)Input (619) â†’ Dense (256) â†’ Dense (128) â†’ Dense (64) â†’ Output (26)



Trains the Hidden Markov Models on the corpus.```| Metric | Value |### Three-Part System



**Steps:**

1. Loads and preprocesses `corpus.txt`

2. Trains 24 separate HMMs for different word lengths**Training Configuration:**|:------:|:-----:|

3. Creates word matcher for pattern filtering

4. Saves models to `hangman_models.pkl` and `word_matcher.pkl`- Experience replay buffer: 10,000 transitions



**Output:**- Target network updates: Every 10 episodes| ğŸ† **Success Rate** | **94.40%** (2000 test games) |**1. Hidden Markov Model (HMM)**

```

HANGMAN HMM TRAINING- Exploration: Epsilon-greedy (1.0 â†’ 0.01)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Loaded 50000 words| ğŸ“‰ **Avg Wrong Guesses** | **2.13** per game |- Position-based states (0, 1, 2, ..., word_length-1)

Training HMMs for each word length...

âœ… HMM Training Complete!### âš¡ Hybrid Strategy

```

| ğŸ² **Repeated Guesses** | **0** |- Letter emissions (A-Z)

#### Part 2: DQN Training (~19 minutes on GPU)

```python

Trains the reinforcement learning agent.

if matching_words <= 20:| ğŸ”„ **Training Win Rate** (Pure RL) | 7.18% |- 24 separate HMMs for word lengths 1-24

**Steps:**

1. Loads trained HMM models    return word_filter_prediction()

2. Creates Hangman environment with reward shaping

3. Trains DQN agent for 5,000 episodeselse:| âœ¨ **Testing Win Rate** (Hybrid) | **94.40%** |- Laplace smoothing (Î±=1.0) for unseen patterns

4. Saves trained agent to `dqn_agent.pth`

    return blend(

**Output:**

```        word_filtering=50%,- Captures positional letter patterns in English

HANGMAN RL TRAINING

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”        hmm_predictions=30%,

Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000

Episode 5000/5000 | Win Rate: 7.18% | Epsilon: 0.01        dqn_q_values=20%</div>

âœ… Training Complete!

```    )



#### Part 3: Evaluation (~2 minutes)```**2. Word Filtering System**



Evaluates the complete hybrid system.



**Steps:**------- Matches current pattern with corpus words

1. Loads all trained models

2. Creates hybrid agent combining HMM, DQN, and word filtering

3. Evaluates on 2,000 test words

4. Generates performance visualizations## ğŸš€ Quick Start- When â‰¤20 words match, uses direct letter frequency

5. Saves results to `evaluation_results.pkl`



**Output:**

```### Installation## ğŸ“ Project Structure- Most powerful component of the system

HANGMAN EVALUATION

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Evaluating Hybrid Agent: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000

Success Rate: 94.40%```bash- Provides accurate predictions for narrow search spaces

Average Wrong Guesses: 2.13

âœ… Evaluation Complete!pip install torch tqdm matplotlib numpy

```

``````

## Performance Analysis



### What Worked

### Data PreparationML-Hackathon/**3. Deep Q-Network (DQN) Agent**

âœ… **Hybrid approach was crucial**

- Pure RL achieved only 7.18% success

- Hybrid system reached 94.40% (13x improvement)

- Different components complemented each other's weaknessesPlace your data files in the `Data/` directory:â”‚- State representation: 619 dimensions



âœ… **Word filtering dominated**- `corpus.txt` - 50,000 training words

- Most powerful individual component

- Direct pattern matching provided the strongest signal- `test.txt` - 2,000 test wordsâ”œâ”€â”€ ğŸ“‚ Data/  - Masked word (540 dims): 20 positions Ã— 27 one-hot features

- Especially effective when search space narrowed down



âœ… **HMM captured language structure**

- Position-specific letter distributions worked well### Running the Notebookâ”‚   â”œâ”€â”€ corpus.txt              # 50,000 training words  - Guessed letters (26 dims): Binary vector

- Helped in early game when pattern matching wasn't sufficient

- Complemented word filtering nicely



### What Didn't WorkThe notebook `Untitled14.ipynb` contains three parts:â”‚   â””â”€â”€ test.txt                # 2,000 test words  - Lives remaining (1 dim): Normalized



âŒ **Pure RL struggled significantly**

- Only 7.18% win rate after 5,000 episodes

- 619-dimensional state space was challenging**Part 1: HMM Training** (~1 minute)â”‚  - HMM probabilities (26 dims)

- Required extensive training to learn basic patterns

- Loads and preprocesses corpus

âŒ **Sparse reward problem**

- Mostly negative feedback during training- Trains 24 HMMs (one per word length)â”œâ”€â”€ ğŸ““ Untitled14.ipynb         # Main implementation notebook  - Word filter probabilities (26 dims)

- Win bonus came too late to effectively guide learning

- Agent struggled to discover good strategies- Creates word matcher for filtering



âŒ **Conservative exploration schedule**- Saves: `hangman_models.pkl`, `word_matcher.pkl`â”œâ”€â”€ ğŸ“„ Analysis_Report.md       # Detailed analysis report- Network architecture: 619 â†’ 256 â†’ 128 â†’ 64 â†’ 26

- Took 700 episodes to reach minimum epsilon

- Could have explored more aggressively early on

- Faster decay might have improved learning

**Part 2: RL Agent Training** (~19 minutes on GPU)â””â”€â”€ ğŸ“– README.md                # This file- Experience replay buffer: 10,000 transitions

### Performance by Word Length

- Loads trained HMMs

| Word Length | Success Rate | Reasoning |

|-------------|--------------|-----------|- Creates Hangman environment```- Target network updated every 10 episodes

| 2-4 letters | 50-80% | Limited context, fewer possible patterns |

| 5-9 letters | 95%+ | Optimal range with enough context |- Trains DQN agent for 5000 episodes

| 10-15 letters | 95%+ | Multiple clues available |

| 16+ letters | ~100% | Extensive context makes guessing easy |- Saves: `dqn_agent.pth`- Epsilon-greedy exploration: 1.0 â†’ 0.01



## Reward Function



The agent uses a carefully designed reward structure:**Part 3: Evaluation** (~2 minutes)---



| Event | Reward | Rationale |- Loads all models

|-------|--------|-----------|

| Correct guess | +10 per revealed position | Encourages productive guesses |- Creates hybrid agent**Hybrid Strategy**

| Win game | +100 | Strong positive reinforcement |

| Wrong guess | -15 | Discourages random guessing |- Evaluates on 2000 test words

| Lose game | -100 | Strong negative feedback |

| Repeated guess | -20 | Heavily penalizes inefficiency |- Saves: `evaluation_results.pkl`, visualizations## ğŸ—ï¸ System Architecture- If matching words â‰¤ 20: Use word filter directly



## Key Insights



### 1. Domain Knowledge Beats Pure Learning### Expected Output- Otherwise: Blend all sources



Explicit word matching with the corpus significantly outperformed the neural network approach. Sometimes, simple rule-based systems can be more effective than complex learning algorithms.



### 2. System Design is Critical```<div align="center">  - 50% word filtering



The intelligent combination of multiple techniques proved more powerful than any single method. The hybrid approach leveraged the strengths of each component:HANGMAN HMM TRAINING

- Word filtering for accuracy

- HMM for language patterns  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  - 30% HMM predictions

- RL for adaptive decision-making

Loaded 50000 words

### 3. RL as Refinement, Not Foundation

Training HMMs for each word length...### **Three-Part Hybrid System**  - 20% DQN Q-values

Deep RL worked best as a refinement tool rather than the primary strategy. Use RL to handle edge cases and combine multiple signals, not to learn the entire task from scratch.

âœ… HMM Training Complete!

### 4. Simple Can Be Better



The straightforward word filtering approach was both simpler to implement and more effective than the complex neural network. Don't overcomplicate solutions without evidence of benefit.

HANGMAN RL TRAINING

## Generated Files

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</div>## ğŸš€ Quick Start

After running all three parts of the notebook, you'll have:

Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000

| File | Description |

|------|-------------|Final Win Rate: 7.18%

| `hangman_models.pkl` | Trained HMM models for all word lengths |

| `word_matcher.pkl` | Word filtering system with corpus index |âœ… Training Complete!

| `dqn_agent.pth` | Trained DQN agent weights |

| `training_results.png` | Training curves and statistics |<br>### Setup

| `evaluation_results.pkl` | Detailed evaluation metrics |

| `evaluation_results.png` | Performance visualizations |HANGMAN EVALUATION



## Technical Referencesâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”



This project implements concepts from:Evaluating Hybrid Agent: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000

- **Hidden Markov Models**: Probabilistic sequence modeling

- **Deep Q-Networks**: Value-based reinforcement learning [(Mnih et al., 2015)](https://www.nature.com/articles/nature14236)Success Rate: 94.40%### ğŸ§  **1. Hidden Markov Model (HMM)**1. **Install dependencies:**

- **Experience Replay**: Stabilizing RL training [(Lin, 1992)](https://link.springer.com/article/10.1007/BF00992699)

- **Target Networks**: Reducing Q-value overestimationâœ… Evaluation Complete!

- **Epsilon-Greedy Exploration**: Balancing exploration and exploitation

``````bash

## Future Improvements



Potential enhancements for even better performance:

---> Captures positional letter patterns in English wordspip install torch tqdm matplotlib numpy pickle

1. **Prioritized Experience Replay**: Sample important transitions more frequently

2. **Double DQN**: Reduce overestimation bias in Q-values

3. **Dueling Network Architecture**: Separate value and advantage streams

4. **Letter Frequency Priors**: Incorporate English letter frequency statistics## ğŸ“Š Key Findings```

5. **Curriculum Learning**: Train on easier words first, gradually increase difficulty

6. **Ensemble Methods**: Combine multiple DQN agents for robustness



## License### âœ… What Worked- **States:** Position-based (0, 1, 2, ..., word_length-1)



This project was developed for educational purposes as part of the PES University Machine Learning Hackathon 2025.



---- **Hybrid approach dominated** - 94.40% vs 7.18% for pure RL- **Emissions:** Letters (A-Z)2. **Prepare data files:**



**PES University - Machine Learning Hackathon 2025**  - **Word filtering was critical** - Direct corpus matching provided strongest signal

**Date:** November 3, 2025

- **HMM captured patterns** - Position-specific letter probabilities worked well- **Models:** 24 separate HMMs for word lengths 1-24- Place `corpus.txt` in root directory (50,000 words)



### âŒ What Didn't Work- **Smoothing:** Laplace smoothing (Î±=1.0) for unseen patterns- Place `test_words.txt` in root directory (2,000 words)



- **Pure RL struggled** - 619-dimensional state space made learning difficult- **Purpose:** Learn where letters typically appear in words

- **Sparse rewards** - Mostly negative feedback hindered learning

- **Conservative exploration** - 700 episodes to reach minimum epsilon was too slow### Running the Notebook



---<br>



## ğŸ¯ Reward FunctionThe notebook `Untitled14.ipynb` contains three main parts:



| Action | Reward |### ğŸ” **2. Word Filtering System**

|--------|--------|

| Correct guess | +10 per position revealed |**Part 1: HMM Training**

| Win game | +100 bonus |

| Wrong guess | -15 penalty |> Direct pattern matching with corpus- Loads and preprocesses corpus

| Lose game | -100 penalty |

| Repeated guess | -20 efficiency penalty |- Trains 24 HMMs (one per word length)



---- Matches current pattern against corpus words- Creates word matcher for filtering



## ğŸ“ˆ Performance by Word Length- When â‰¤20 words match â†’ uses direct letter frequency- Saves models: `hangman_models.pkl`, `word_matcher.pkl`



| Word Length | Win Rate | Context |- **Most powerful component** of the system- Training time: ~1 minute

|-------------|----------|---------|

| 2-4 letters | 50-80% | Limited context |- Provides highly accurate predictions for narrow search spaces

| 5-9 letters | 95%+ | Optimal range |

| 10-15 letters | 95%+ | Many clues |**Part 2: RL Agent Training**

| 16+ letters | ~100% | Extensive context |

<br>- Loads trained HMMs

---

- Creates Hangman environment

## ğŸ”§ Training Parameters

### ğŸ¤– **3. Deep Q-Network (DQN) Agent**- Trains DQN agent for 5000 episodes

| Parameter | Value |

|-----------|-------|- Saves model: `dqn_agent.pth`

| Episodes | 5000 |

| Batch Size | 64 |> Reinforcement learning for strategic decision-making- Training time: ~19 minutes on GPU

| Learning Rate | 0.001 |

| Gamma (Discount) | 0.95 |

| Epsilon Start | 1.0 |

| Epsilon Min | 0.01 |**State Representation (619 dimensions):****Part 3: Evaluation**

| Epsilon Decay | 0.995 |

| Replay Buffer | 10,000 |- ğŸ¯ Masked word (540 dims): 20 positions Ã— 27 one-hot features- Loads all models

| Target Update | Every 10 episodes |

- âœ… Guessed letters (26 dims): Binary vector- Creates hybrid agent

---

- â¤ï¸ Lives remaining (1 dim): Normalized- Evaluates on 2000 test words

## ğŸ“ Generated Files

- ğŸ“Š HMM probabilities (26 dims)- Generates visualizations

- `hangman_models.pkl` - Trained HMM models for all word lengths

- `word_matcher.pkl` - Word filtering system- ğŸ“ˆ Word filter probabilities (26 dims)- Saves results: `evaluation_results.pkl`

- `dqn_agent.pth` - Trained DQN agent weights

- `training_results.png` - Training curves visualization- Evaluation time: ~2 minutes

- `evaluation_results.pkl` - Evaluation statistics

- `evaluation_results.png` - Evaluation visualizations**Neural Network:**



---```### Expected Output



## ğŸ’¡ Key Lessons619 â†’ 256 â†’ 128 â†’ 64 â†’ 26



1. **Domain knowledge beats pure learning** - Explicit word matching outperformed neural networks``````

2. **System design matters** - Intelligent combination of techniques > any single method

3. **RL as refinement** - Use RL to handle edge cases, not learn entire strategyHANGMAN HMM TRAINING

4. **Simple can be better** - Word filtering was simpler AND more effective than complex neural networks

**Training Features:**Loaded 50000 words

---

- Experience replay buffer: 10,000 transitionsTraining HMMs for each word length...

## ğŸ“š References

- Target network updates: Every 10 episodesâœ… HMM Training Complete!

- Hidden Markov Models for sequence prediction

- Deep Q-Networks (DQN) for reinforcement learning- Exploration: Epsilon-greedy (1.0 â†’ 0.01)

- Experience replay and target networks for stable learning

- Epsilon-greedy exploration strategyHANGMAN RL TRAINING



---<br>Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000



**PES University - Machine Learning Hackathon 2025**  Final Win Rate: 7.18%

*November 3, 2025*

### âš¡ **Hybrid Strategy**âœ… Training Complete!



```pythonHANGMAN EVALUATION

if matching_words <= 20:Evaluating Hybrid Agent: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000

    return word_filter_prediction()Success Rate: 94.40%

else:âœ… Evaluation Complete!

    return blend(```

        word_filtering=50%,

        hmm_predictions=30%,## ğŸ“Š Key Findings

        dqn_q_values=20%

    )### What Worked

```

âœ… **Hybrid approach dominated** - Combining word filtering, HMM, and RL achieved 94.40% vs 7.18% for pure RL

---

âœ… **Word filtering was critical** - Direct corpus matching provided the strongest signal

## ğŸš€ Quick Start Guide

âœ… **HMM captured language patterns** - Position-specific letter probabilities worked well

### ğŸ“¦ **Installation**

### What Didn't Work

```bash

pip install torch tqdm matplotlib numpyâŒ **Pure RL struggled** - 619-dimensional state space made learning difficult

```

âŒ **Sparse rewards** - Mostly negative feedback hindered learning

### ğŸ“‚ **Data Preparation**

âŒ **Too conservative exploration** - 700 episodes to reach minimum epsilon was too slow

Place your data files in the `Data/` directory:

- `corpus.txt` - 50,000 training words## ğŸ¯ Reward Function

- `test.txt` - 2,000 test words

- **Correct guess:** +10 per position revealed + 100 if won

### â–¶ï¸ **Running the Notebook**- **Wrong guess:** -15 (+ -100 if lost)

- **Repeated guess:** -20 (heavy efficiency penalty)

The notebook `Untitled14.ipynb` has three main sections:

## ï¿½ Performance by Word Length

<br>

- **Short words (2-4 letters):** ~50-80% win rate (limited context)

#### **Part 1: ğŸ§  HMM Training** (~1 minute)- **Medium words (5-9 letters):** ~95%+ win rate (optimal)

- Loads and preprocesses corpus- **Long words (10-15 letters):** ~95%+ win rate (many clues)

- Trains 24 HMMs (one per word length)- **Very long words (16+ letters):** ~100% win rate (extensive context)

- Creates word matcher for filtering

- **Saves:** `hangman_models.pkl`, `word_matcher.pkl`## ğŸ”§ Training Parameters



#### **Part 2: ğŸ¤– RL Agent Training** (~19 minutes on GPU)| Parameter | Value |

- Loads trained HMMs|-----------|-------|

- Creates Hangman environment| Episodes | 5000 |

- Trains DQN agent for 5000 episodes| Batch Size | 64 |

- **Saves:** `dqn_agent.pth`| Learning Rate | 0.001 |

| Gamma (Discount) | 0.95 |

#### **Part 3: ğŸ“Š Evaluation** (~2 minutes)| Epsilon Start | 1.0 |

- Loads all models| Epsilon Min | 0.01 |

- Creates hybrid agent| Epsilon Decay | 0.995 |

- Evaluates on 2000 test words| Replay Buffer | 10,000 |

- **Saves:** `evaluation_results.pkl`, visualizations| Target Update | Every 10 episodes |



<br>## ğŸ“ Files Generated



### ğŸ“º **Expected Output**- `hangman_models.pkl` - Trained HMM models for all word lengths

- `word_matcher.pkl` - Word filtering system

```- `dqn_agent.pth` - Trained DQN agent weights

ğŸ§  HANGMAN HMM TRAINING- `training_results.png` - Training curves visualization

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”- `evaluation_results.pkl` - Evaluation statistics

Loaded 50000 words- `evaluation_results.png` - Evaluation visualizations

Training HMMs for each word length...

âœ… HMM Training Complete!## ï¿½ Key Lessons



ğŸ¤– HANGMAN RL TRAINING1. **Domain knowledge beats pure learning** - Explicit word matching outperformed neural networks

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”2. **System design matters** - Intelligent combination of techniques is more powerful than any single method

Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/50003. **RL as refinement** - Use RL to handle edge cases, not learn entire strategy from scratch

Final Win Rate: 7.18%4. **Simple can be better** - Word filtering was simpler and more effective than complex neural networks

âœ… Training Complete!

## ï¿½ References

ğŸ“Š HANGMAN EVALUATION

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”- Hidden Markov Models for sequence prediction

Evaluating Hybrid Agent: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000- Deep Q-Networks (DQN) for reinforcement learning

Success Rate: 94.40%- Experience replay and target networks for stable learning

âœ… Evaluation Complete!- Epsilon-greedy exploration strategy

```

---

---

**PES University - Machine Learning Hackathon 2025**  

## ğŸ“Š Key Findings**Date:** November 3, 2025


<table>
<tr>
<td width="50%">

### âœ… **What Worked**

- **ğŸ† Hybrid approach dominated**  
  94.40% vs 7.18% for pure RL

- **ğŸ” Word filtering was critical**  
  Direct corpus matching = strongest signal

- **ğŸ§  HMM captured patterns**  
  Position-specific letter probabilities

</td>
<td width="50%">

### âŒ **What Didn't Work**

- **ğŸ¤– Pure RL struggled**  
  619-dimensional state space too complex

- **âš ï¸ Sparse rewards**  
  Mostly negative feedback hindered learning

- **ğŸŒ Conservative exploration**  
  700 episodes to reach min epsilon

</td>
</tr>
</table>

---

## ğŸ¯ Reward Function

| Action | Reward |
|:-------|:------:|
| âœ… Correct guess | **+10** per position revealed |
| ğŸŠ Win game | **+100** bonus |
| âŒ Wrong guess | **-15** penalty |
| ğŸ’€ Lose game | **-100** penalty |
| ğŸ” Repeated guess | **-20** efficiency penalty |

---

## ğŸ“ˆ Performance by Word Length

<div align="center">

| Word Length | Win Rate | Context Level |
|:-----------:|:--------:|:-------------:|
| 2-4 letters | 50-80% | âš ï¸ Limited |
| 5-9 letters | **95%+** | âœ… Optimal |
| 10-15 letters | **95%+** | âœ… Many clues |
| 16+ letters | **~100%** | ğŸ¯ Extensive |

</div>

---

## ğŸ”§ Training Parameters

<div align="center">

| Parameter | Value |
|:---------:|:-----:|
| ğŸ”„ **Episodes** | 5000 |
| ğŸ“¦ **Batch Size** | 64 |
| ğŸ“š **Learning Rate** | 0.001 |
| ğŸ’° **Gamma (Discount)** | 0.95 |
| ğŸ² **Epsilon Start** | 1.0 |
| ğŸ¯ **Epsilon Min** | 0.01 |
| ğŸ“‰ **Epsilon Decay** | 0.995 |
| ğŸ’¾ **Replay Buffer** | 10,000 |
| ğŸ”„ **Target Update** | Every 10 episodes |

</div>

---

## ğŸ“ Generated Files

| File | Description |
|:-----|:------------|
| ğŸ§  `hangman_models.pkl` | Trained HMM models for all word lengths |
| ğŸ” `word_matcher.pkl` | Word filtering system |
| ğŸ¤– `dqn_agent.pth` | Trained DQN agent weights |
| ğŸ“Š `training_results.png` | Training curves visualization |
| ğŸ“ˆ `evaluation_results.pkl` | Evaluation statistics |
| ğŸ¨ `evaluation_results.png` | Evaluation visualizations |

---

## ğŸ’¡ Key Lessons Learned

<div align="center">

### ğŸŒŸ **Four Core Insights**

</div>

<br>

> **1. ğŸ§  Domain knowledge beats pure learning**  
> Explicit word matching outperformed neural networks

> **2. ğŸ—ï¸ System design matters**  
> Intelligent combination > any single method

> **3. âš¡ RL as refinement**  
> Use RL for edge cases, not learning entire strategy

> **4. âœ¨ Simple can be better**  
> Word filtering was simpler AND more effective

---

## ğŸ“š References

- ğŸ§  Hidden Markov Models for sequence prediction
- ğŸ¤– Deep Q-Networks (DQN) for reinforcement learning
- ğŸ’¾ Experience replay and target networks
- ğŸ² Epsilon-greedy exploration strategy

---

<div align="center">

### ğŸ“ **PES University**
**Machine Learning Hackathon 2025**

*Date: November 3, 2025*

<br>

**Made with â¤ï¸ by Team Mohammed**

</div>
