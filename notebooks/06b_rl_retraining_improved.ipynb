{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6f4c0c",
   "metadata": {},
   "source": [
    "# Improved RL Training with Better Hyperparameters\n",
    "\n",
    "This notebook retrains the RL agent with:\n",
    "- More training episodes (10,000)\n",
    "- Slower epsilon decay for better exploration\n",
    "- Higher learning rate\n",
    "- Adjusted reward function alignment with scoring formula\n",
    "\n",
    "**Expected time on M1 Pro:** ~30-45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from rl_agent import QLearningAgent\n",
    "from hmm_model import HangmanHMM\n",
    "from hangman_env import HangmanEnv\n",
    "from utils import encode_state\n",
    "\n",
    "# Load HMM model\n",
    "hmm = HangmanHMM()\n",
    "hmm.load('../models/hmm_model.pkl')\n",
    "print(\"HMM model loaded\")\n",
    "\n",
    "# Load corpus words\n",
    "with open('../results/preprocessed_data.pkl', 'rb') as f:\n",
    "    preprocessed_data = pickle.load(f)\n",
    "\n",
    "corpus_words = preprocessed_data['words']\n",
    "print(f\"Loaded {len(corpus_words)} words from corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520792a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RL agent with IMPROVED hyperparameters\n",
    "agent = QLearningAgent(\n",
    "    learning_rate=0.15,        # Increased from 0.1\n",
    "    discount_factor=0.95,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.9998,      # Slower decay from 0.9995\n",
    "    epsilon_min=0.02,          # Lower minimum for more exploitation\n",
    "    max_word_length=30\n",
    ")\n",
    "\n",
    "print(\"RL agent initialized with IMPROVED hyperparameters\")\n",
    "print(f\"Learning rate: {agent.learning_rate}\")\n",
    "print(f\"Initial epsilon: {agent.epsilon}\")\n",
    "print(f\"Epsilon decay: {agent.epsilon_decay}\")\n",
    "print(f\"Min epsilon: {agent.epsilon_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6037d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters - MORE EPISODES\n",
    "num_episodes = 10000       # Increased from 3000\n",
    "eval_interval = 1000       # Evaluate every 1000 episodes\n",
    "eval_episodes = 100\n",
    "\n",
    "# Training statistics\n",
    "training_history = {\n",
    "    'episode_rewards': [],\n",
    "    'episode_wins': [],\n",
    "    'episode_wrong_guesses': [],\n",
    "    'epsilon_values': [],\n",
    "    'eval_win_rates': [],\n",
    "    'eval_avg_rewards': []\n",
    "}\n",
    "\n",
    "print(f\"Training for {num_episodes} episodes...\")\n",
    "print(f\"Expected time: ~45 minutes on M1 Pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_episode(agent, hmm, word):\n",
    "    \"\"\"Train agent on a single episode.\"\"\"\n",
    "    env = HangmanEnv(word, max_lives=6)\n",
    "    state = env.reset()\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    episode_wrong = 0\n",
    "    \n",
    "    prev_state_features = None\n",
    "    prev_hmm_probs = None\n",
    "    prev_action = None\n",
    "    prev_reward = 0.0\n",
    "    \n",
    "    while not env.done:\n",
    "        masked_list = env.get_masked_word_list()\n",
    "        hmm_probs = hmm.predict_letter_probabilities(masked_list, env.guessed_letters, len(word))\n",
    "        state_features = encode_state(masked_list, env.guessed_letters, hmm_probs,\n",
    "                                     env.lives, len(word))\n",
    "        \n",
    "        action = agent.select_action(state_features, hmm_probs, env.guessed_letters)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if not info.get('correct', False) and not info.get('repeated', False):\n",
    "            episode_wrong += 1\n",
    "        \n",
    "        if prev_state_features is not None:\n",
    "            agent.update_q_value(\n",
    "                prev_state_features,\n",
    "                prev_action,\n",
    "                prev_reward,\n",
    "                state_features,\n",
    "                hmm_probs,\n",
    "                env.guessed_letters,\n",
    "                done,\n",
    "                current_hmm_probs=prev_hmm_probs\n",
    "            )\n",
    "        \n",
    "        prev_state_features = state_features\n",
    "        prev_hmm_probs = hmm_probs\n",
    "        prev_action = action\n",
    "        prev_reward = reward\n",
    "        \n",
    "        if done:\n",
    "            agent.update_q_value(\n",
    "                prev_state_features,\n",
    "                prev_action,\n",
    "                prev_reward,\n",
    "                None,\n",
    "                None,\n",
    "                None,\n",
    "                done,\n",
    "                current_hmm_probs=prev_hmm_probs\n",
    "            )\n",
    "            break\n",
    "    \n",
    "    return episode_reward, env.won, episode_wrong\n",
    "\n",
    "print(\"Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING IMPROVED RL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training episodes\"):\n",
    "    # Sample a random word\n",
    "    word = random.choice(corpus_words)\n",
    "    \n",
    "    # Train on this episode\n",
    "    reward, won, wrong = train_episode(agent, hmm, word)\n",
    "    \n",
    "    # Record statistics\n",
    "    training_history['episode_rewards'].append(reward)\n",
    "    training_history['episode_wins'].append(1 if won else 0)\n",
    "    training_history['episode_wrong_guesses'].append(wrong)\n",
    "    training_history['epsilon_values'].append(agent.epsilon)\n",
    "    \n",
    "    # Decay epsilon\n",
    "    agent.decay_epsilon()\n",
    "    \n",
    "    # Periodic evaluation\n",
    "    if (episode + 1) % eval_interval == 0:\n",
    "        # Evaluate on random sample\n",
    "        eval_words = random.sample(corpus_words, eval_episodes)\n",
    "        eval_rewards = []\n",
    "        eval_wins = 0\n",
    "        \n",
    "        old_epsilon = agent.epsilon\n",
    "        agent.epsilon = 0.0  # Pure exploitation for eval\n",
    "        \n",
    "        for eval_word in eval_words:\n",
    "            r, w, _ = train_episode(agent, hmm, eval_word)\n",
    "            eval_rewards.append(r)\n",
    "            if w:\n",
    "                eval_wins += 1\n",
    "        \n",
    "        agent.epsilon = old_epsilon\n",
    "        \n",
    "        win_rate = eval_wins / eval_episodes\n",
    "        avg_reward = np.mean(eval_rewards)\n",
    "        \n",
    "        training_history['eval_win_rates'].append(win_rate)\n",
    "        training_history['eval_avg_rewards'].append(avg_reward)\n",
    "        \n",
    "        print(f\"\\n[Episode {episode+1}]\")\n",
    "        print(f\"  Eval Win Rate: {win_rate:.4f} ({win_rate*100:.2f}%)\")\n",
    "        print(f\"  Eval Avg Reward: {avg_reward:.2f}\")\n",
    "        print(f\"  Current Epsilon: {agent.epsilon:.4f}\")\n",
    "        print(f\"  Q-table size: {len(agent.q_table)} states\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the improved model\n",
    "agent.save('../models/rl_agent_improved.pkl')\n",
    "print(\"✓ Improved RL agent saved to ../models/rl_agent_improved.pkl\")\n",
    "\n",
    "# Also save training history\n",
    "with open('../results/training_history_improved.pkl', 'wb') as f:\n",
    "    pickle.dump(training_history, f)\n",
    "print(\"✓ Training history saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fea0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Episode rewards (moving average)\n",
    "window = 100\n",
    "rewards_ma = np.convolve(training_history['episode_rewards'], \n",
    "                        np.ones(window)/window, mode='valid')\n",
    "axes[0, 0].plot(rewards_ma)\n",
    "axes[0, 0].set_title('Episode Rewards (100-episode moving average)')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot 2: Win rate (moving average)\n",
    "wins_ma = np.convolve(training_history['episode_wins'], \n",
    "                     np.ones(window)/window, mode='valid')\n",
    "axes[0, 1].plot(wins_ma)\n",
    "axes[0, 1].set_title('Win Rate (100-episode moving average)')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Win Rate')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot 3: Epsilon decay\n",
    "axes[1, 0].plot(training_history['epsilon_values'])\n",
    "axes[1, 0].set_title('Epsilon Decay')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Epsilon')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot 4: Evaluation win rates\n",
    "eval_episodes_x = list(range(eval_interval, num_episodes+1, eval_interval))\n",
    "axes[1, 1].plot(eval_episodes_x, training_history['eval_win_rates'], 'o-')\n",
    "axes[1, 1].set_title('Evaluation Win Rate Over Training')\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Win Rate')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/training_progress_improved.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training plots saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d6250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL TRAINING STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total episodes trained: {num_episodes}\")\n",
    "print(f\"Final epsilon: {agent.epsilon:.4f}\")\n",
    "print(f\"Q-table size: {len(agent.q_table)} states\")\n",
    "print(f\"Total state-action pairs: {sum(len(actions) for actions in agent.q_table.values())}\")\n",
    "\n",
    "# Last 1000 episodes statistics\n",
    "last_1000_wins = sum(training_history['episode_wins'][-1000:])\n",
    "last_1000_win_rate = last_1000_wins / 1000\n",
    "print(f\"\\nLast 1000 episodes:\")\n",
    "print(f\"  Win rate: {last_1000_win_rate:.4f} ({last_1000_win_rate*100:.2f}%)\")\n",
    "print(f\"  Avg reward: {np.mean(training_history['episode_rewards'][-1000:]):.2f}\")\n",
    "print(f\"  Avg wrong guesses: {np.mean(training_history['episode_wrong_guesses'][-1000:]):.2f}\")\n",
    "\n",
    "if training_history['eval_win_rates']:\n",
    "    print(f\"\\nBest evaluation win rate: {max(training_history['eval_win_rates']):.4f}\")\n",
    "    print(f\"Final evaluation win rate: {training_history['eval_win_rates'][-1]:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455371a",
   "metadata": {},
   "source": [
    "## Next Step: Evaluate the Improved Model\n",
    "\n",
    "Now go back to `07_evaluation.ipynb` and:\n",
    "1. Change the model loading line to: `agent.load('../models/rl_agent_improved.pkl')`\n",
    "2. Re-run the evaluation cells\n",
    "3. Compare the new results!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
