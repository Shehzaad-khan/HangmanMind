{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e77e4f3",
   "metadata": {},
   "source": [
    "# Proper RL Training from Scratch\n",
    "\n",
    "**Goal:** Train a PROPER RL agent that learns to use the improved HMM effectively\n",
    "\n",
    "Strategy: Train Q-learning agent that combines learned Q-values with improved HMM probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8ce44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded improved HMM\n",
      "âœ“ Loaded 50000 training words\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from hangman_env import HangmanEnv\n",
    "from utils import calculate_final_score\n",
    "\n",
    "# Define ImprovedHMM class (needed to load the pickle)\n",
    "class ImprovedHMM:\n",
    "    def __init__(self):\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.global_freq = Counter()\n",
    "        self.bigrams = defaultdict(Counter)\n",
    "        self.position_freq = defaultdict(Counter)\n",
    "        self.length_patterns = defaultdict(Counter)\n",
    "    \n",
    "    def predict_letter_probabilities(self, masked_word, guessed_letters, word_length):\n",
    "        \"\"\"Predict letter probabilities for current state.\"\"\"\n",
    "        probs = {c: 0.0 for c in self.alphabet}\n",
    "        \n",
    "        # Strategy 1: Global frequency\n",
    "        for char in self.alphabet:\n",
    "            if char not in guessed_letters:\n",
    "                probs[char] += self.global_freq.get(char, 0.0) * 1.0\n",
    "        \n",
    "        # Strategy 2: Bigrams\n",
    "        for i, char in enumerate(masked_word):\n",
    "            if char is not None:\n",
    "                if i + 1 < len(masked_word) and masked_word[i+1] is None:\n",
    "                    if char in self.bigrams:\n",
    "                        total = sum(self.bigrams[char].values())\n",
    "                        if total > 0:\n",
    "                            for next_char, count in self.bigrams[char].items():\n",
    "                                if next_char not in guessed_letters:\n",
    "                                    probs[next_char] += (count / total) * 2.0\n",
    "                \n",
    "                if i > 0 and masked_word[i-1] is None:\n",
    "                    for prev_char in self.alphabet:\n",
    "                        if prev_char not in guessed_letters and prev_char in self.bigrams:\n",
    "                            if char in self.bigrams[prev_char]:\n",
    "                                count = self.bigrams[prev_char][char]\n",
    "                                total = sum(self.bigrams[prev_char].values())\n",
    "                                if total > 0:\n",
    "                                    probs[prev_char] += (count / total) * 2.0\n",
    "        \n",
    "        # Strategy 3: Position frequency\n",
    "        for i, char in enumerate(masked_word):\n",
    "            if char is None and i < 20:\n",
    "                if i in self.position_freq:\n",
    "                    total = sum(self.position_freq[i].values())\n",
    "                    if total > 0:\n",
    "                        for c, count in self.position_freq[i].items():\n",
    "                            if c not in guessed_letters:\n",
    "                                probs[c] += (count / total) * 1.5\n",
    "        \n",
    "        # Strategy 4: Length patterns\n",
    "        if word_length in self.length_patterns:\n",
    "            total = sum(self.length_patterns[word_length].values())\n",
    "            if total > 0:\n",
    "                for c, count in self.length_patterns[word_length].items():\n",
    "                    if c not in guessed_letters:\n",
    "                        probs[c] += (count / total) * 0.5\n",
    "        \n",
    "        # Normalize\n",
    "        total = sum(probs.values())\n",
    "        if total > 0:\n",
    "            probs = {c: p/total for c, p in probs.items()}\n",
    "        \n",
    "        return probs\n",
    "\n",
    "# Load the improved HMM\n",
    "with open('../models/improved_hmm.pkl', 'rb') as f:\n",
    "    hmm = pickle.load(f)\n",
    "\n",
    "print(\"âœ“ Loaded improved HMM\")\n",
    "\n",
    "# Load corpus for training\n",
    "with open('../Data/corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus_words = [line.strip().lower() for line in f if line.strip()]\n",
    "corpus_words = [''.join(c for c in word if c.isalpha()) for word in corpus_words]\n",
    "corpus_words = [w for w in corpus_words if len(w) > 0]\n",
    "\n",
    "print(f\"âœ“ Loaded {len(corpus_words)} training words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83afce87",
   "metadata": {},
   "source": [
    "## Define Improved RL Agent\n",
    "\n",
    "Key improvements:\n",
    "1. Better state representation\n",
    "2. Optimized hyperparameters\n",
    "3. Better exploration strategy\n",
    "4. Adaptive HMM weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7911b3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ImprovedRLAgent class defined\n"
     ]
    }
   ],
   "source": [
    "class ImprovedRLAgent:\n",
    "    def __init__(self, hmm, alphabet='abcdefghijklmnopqrstuvwxyz'):\n",
    "        self.alphabet = alphabet\n",
    "        self.hmm = hmm\n",
    "        \n",
    "        # Q-table\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(alphabet)))\n",
    "        \n",
    "        # Hyperparameters - optimized\n",
    "        self.alpha = 0.1  # Learning rate\n",
    "        self.gamma = 0.9  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate (starts high)\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        # HMM weight - adaptive\n",
    "        self.hmm_weight_start = 2.0  # Start with high HMM influence\n",
    "        self.hmm_weight_end = 1.0    # End with balanced approach\n",
    "        self.hmm_weight = self.hmm_weight_start\n",
    "    \n",
    "    def get_state(self, env):\n",
    "        \"\"\"Enhanced state representation.\"\"\"\n",
    "        masked = env.get_masked_word()\n",
    "        lives = env.lives\n",
    "        word_len = len(env.word)\n",
    "        guessed_count = len(env.guessed_letters)\n",
    "        \n",
    "        # Create state tuple\n",
    "        state = (masked, lives, word_len, guessed_count)\n",
    "        return state\n",
    "    \n",
    "    def select_action(self, state, env, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy with HMM prior.\"\"\"\n",
    "        # Exploration\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            available = [c for c in self.alphabet if c not in env.guessed_letters]\n",
    "            if available:\n",
    "                return np.random.choice(available)\n",
    "            return None\n",
    "        \n",
    "        # Exploitation: Q-values + HMM probabilities\n",
    "        q_values = self.q_table[state].copy()\n",
    "        \n",
    "        # Get HMM probabilities\n",
    "        masked_list = env.get_masked_word_list()\n",
    "        hmm_probs = self.hmm.predict_letter_probabilities(\n",
    "            masked_list, \n",
    "            env.guessed_letters, \n",
    "            len(env.word)\n",
    "        )\n",
    "        \n",
    "        # Combine Q-values and HMM probabilities\n",
    "        action_values = np.zeros(len(self.alphabet))\n",
    "        for i, char in enumerate(self.alphabet):\n",
    "            if char not in env.guessed_letters:\n",
    "                action_values[i] = q_values[i] + self.hmm_weight * hmm_probs.get(char, 0.0)\n",
    "            else:\n",
    "                action_values[i] = -np.inf\n",
    "        \n",
    "        if np.all(action_values == -np.inf):\n",
    "            return None\n",
    "        \n",
    "        action_idx = np.argmax(action_values)\n",
    "        return self.alphabet[action_idx]\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value using Q-learning.\"\"\"\n",
    "        if action is None:\n",
    "            return\n",
    "        \n",
    "        action_idx = self.alphabet.index(action)\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[state][action_idx]\n",
    "        \n",
    "        # Max Q-value for next state\n",
    "        if done:\n",
    "            max_next_q = 0\n",
    "        else:\n",
    "            max_next_q = np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Q-learning update\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state][action_idx] = new_q\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def update_hmm_weight(self, progress):\n",
    "        \"\"\"Gradually reduce HMM weight as agent learns.\"\"\"\n",
    "        self.hmm_weight = self.hmm_weight_start + progress * (self.hmm_weight_end - self.hmm_weight_start)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save agent.\"\"\"\n",
    "        data = {\n",
    "            'q_table': dict(self.q_table),\n",
    "            'epsilon': self.epsilon,\n",
    "            'hmm_weight': self.hmm_weight\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"âœ“ Agent saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load agent.\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(self.alphabet)), data['q_table'])\n",
    "        self.epsilon = data['epsilon']\n",
    "        self.hmm_weight = data['hmm_weight']\n",
    "        print(f\"âœ“ Agent loaded from {filepath}\")\n",
    "\n",
    "print(\"âœ“ ImprovedRLAgent class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e40098",
   "metadata": {},
   "source": [
    "## Train RL Agent\n",
    "\n",
    "Train on corpus words with proper reward shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b3c750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RL agent for 5000 episodes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆ        | 1025/5000 [00:07<00:27, 142.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1000:\n",
      "  Win Rate (last 1000): 16.40%\n",
      "  Avg Reward: 0.15\n",
      "  Epsilon: 0.0100\n",
      "  HMM Weight: 1.80\n",
      "  Q-table size: 8134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2028/5000 [00:14<00:21, 141.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 2000:\n",
      "  Win Rate (last 1000): 19.80%\n",
      "  Avg Reward: 1.43\n",
      "  Epsilon: 0.0100\n",
      "  HMM Weight: 1.60\n",
      "  Q-table size: 15993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3027/5000 [00:21<00:13, 141.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 3000:\n",
      "  Win Rate (last 1000): 20.60%\n",
      "  Avg Reward: 1.78\n",
      "  Epsilon: 0.0100\n",
      "  HMM Weight: 1.40\n",
      "  Q-table size: 23631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4015/5000 [00:28<00:06, 140.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 4000:\n",
      "  Win Rate (last 1000): 20.80%\n",
      "  Avg Reward: 1.93\n",
      "  Epsilon: 0.0100\n",
      "  HMM Weight: 1.20\n",
      "  Q-table size: 30992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:35<00:00, 141.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 5000:\n",
      "  Win Rate (last 1000): 23.80%\n",
      "  Avg Reward: 2.38\n",
      "  Epsilon: 0.0100\n",
      "  HMM Weight: 1.00\n",
      "  Q-table size: 38155\n",
      "\n",
      "âœ… Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_agent(agent, train_words, episodes=5000):\n",
    "    \"\"\"Train the RL agent.\"\"\"\n",
    "    print(f\"Training RL agent for {episodes} episodes...\\n\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    win_rates = []\n",
    "    \n",
    "    for episode in tqdm(range(episodes), desc=\"Training\"):\n",
    "        # Sample random word\n",
    "        word = np.random.choice(train_words)\n",
    "        env = HangmanEnv(word, max_lives=6)\n",
    "        state = agent.get_state(env)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not env.done:\n",
    "            # Select action\n",
    "            action = agent.select_action(state, env, training=True)\n",
    "            \n",
    "            if action is None:\n",
    "                break\n",
    "            \n",
    "            # Take action - returns (observation, reward, done, info)\n",
    "            _, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Get next state\n",
    "            next_state = agent.get_state(env)\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state, env.done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        win_rates.append(1 if env.won else 0)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Update HMM weight\n",
    "        progress = episode / episodes\n",
    "        agent.update_hmm_weight(progress)\n",
    "        \n",
    "        # Log progress\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            recent_win_rate = np.mean(win_rates[-1000:])\n",
    "            recent_reward = np.mean(episode_rewards[-1000:])\n",
    "            print(f\"\\nEpisode {episode + 1}:\")\n",
    "            print(f\"  Win Rate (last 1000): {recent_win_rate:.2%}\")\n",
    "            print(f\"  Avg Reward: {recent_reward:.2f}\")\n",
    "            print(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "            print(f\"  HMM Weight: {agent.hmm_weight:.2f}\")\n",
    "            print(f\"  Q-table size: {len(agent.q_table)}\")\n",
    "    \n",
    "    print(\"\\nâœ… Training complete!\")\n",
    "    return episode_rewards, win_rates\n",
    "\n",
    "# Create and train agent\n",
    "agent = ImprovedRLAgent(hmm)\n",
    "episode_rewards, win_rates = train_agent(agent, corpus_words, episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db62e9",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0006c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATING IMPROVED RL AGENT (500 test words)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 1476.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Win Rate: 0.2360 (23.60%)\n",
      "Wrong Guesses: 2728 (avg: 5.46)\n",
      "Repeated: 0\n",
      "Score: -13522.00\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load test words\n",
    "with open('../Data/test.txt', 'r') as f:\n",
    "    test_words = [''.join(c for c in line.strip().lower() if c.isalpha()) for line in f if line.strip()][:500]\n",
    "\n",
    "def evaluate_agent(agent, test_words):\n",
    "    \"\"\"Evaluate agent on test set.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for word in tqdm(test_words, desc=\"Evaluating\"):\n",
    "        env = HangmanEnv(word, max_lives=6)\n",
    "        state = agent.get_state(env)\n",
    "        \n",
    "        while not env.done:\n",
    "            action = agent.select_action(state, env, training=False)\n",
    "            if action is None:\n",
    "                break\n",
    "            env.step(action)\n",
    "            state = agent.get_state(env)\n",
    "        \n",
    "        stats = env.get_stats()\n",
    "        results.append({\n",
    "            'won': env.won,\n",
    "            'wrong': stats['wrong_count'],\n",
    "            'repeated': stats['repeated_count']\n",
    "        })\n",
    "    \n",
    "    wins = sum(1 for r in results if r['won'])\n",
    "    rate = wins / len(results)\n",
    "    wrong = sum(r['wrong'] for r in results)\n",
    "    repeated = sum(r['repeated'] for r in results)\n",
    "    score = calculate_final_score(rate, wrong, repeated, len(results))\n",
    "    \n",
    "    return rate, score, wrong, repeated\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING IMPROVED RL AGENT (500 test words)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rate, score, wrong, repeated = evaluate_agent(agent, test_words)\n",
    "\n",
    "print(f\"\\nWin Rate: {rate:.4f} ({rate*100:.2f}%)\")\n",
    "print(f\"Wrong Guesses: {wrong} (avg: {wrong/len(test_words):.2f})\")\n",
    "print(f\"Repeated: {repeated}\")\n",
    "print(f\"Score: {score:.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013e4f82",
   "metadata": {},
   "source": [
    "## Save Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3735a30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Agent saved to ../models/improved_rl_agent.pkl\n",
      "\n",
      "âœ… Improved RL agent saved!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained agent\n",
    "agent.save('../models/improved_rl_agent.pkl')\n",
    "print(\"\\nâœ… Improved RL agent saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cca894",
   "metadata": {},
   "source": [
    "## Final Evaluation on Full Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f700d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running FINAL evaluation on all 2000 test words...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1555.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL RESULTS - IMPROVED RL + IMPROVED HMM\n",
      "======================================================================\n",
      "Win Rate: 0.2110 (21.10%)\n",
      "Total Wrong Guesses: 11023\n",
      "Total Repeated: 0\n",
      "\n",
      "ðŸŽ¯ FINAL SCORE: -54693.00\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Comparison:\n",
      "  Previous best: 19.25% win rate, -55,325 score\n",
      "  Current:       21.10% win rate, -54693.00 score\n",
      "  Improvement:   +1.85 percentage points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load ALL test words\n",
    "with open('../Data/test.txt', 'r') as f:\n",
    "    all_test_words = [''.join(c for c in line.strip().lower() if c.isalpha()) for line in f if line.strip()]\n",
    "\n",
    "print(f\"\\nRunning FINAL evaluation on all {len(all_test_words)} test words...\\n\")\n",
    "\n",
    "rate, score, wrong, repeated = evaluate_agent(agent, all_test_words)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS - IMPROVED RL + IMPROVED HMM\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Win Rate: {rate:.4f} ({rate*100:.2f}%)\")\n",
    "print(f\"Total Wrong Guesses: {wrong}\")\n",
    "print(f\"Total Repeated: {repeated}\")\n",
    "print(f\"\\nðŸŽ¯ FINAL SCORE: {score:.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\nðŸ“Š Comparison:\")\n",
    "print(f\"  Previous best: 19.25% win rate, -55,325 score\")\n",
    "print(f\"  Current:       {rate*100:.2f}% win rate, {score:.2f} score\")\n",
    "improvement = rate * 100 - 19.25\n",
    "print(f\"  Improvement:   {improvement:+.2f} percentage points\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
