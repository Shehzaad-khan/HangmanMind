{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c6010b",
   "metadata": {},
   "source": [
    "# Simple Neural HMM for Hangman\n",
    "\n",
    "**No DQN - Just a well-trained Neural Network**\n",
    "\n",
    "## Why Simple?\n",
    "- âœ… Dataset is small (50K words) - perfect for supervised learning\n",
    "- âœ… Direct letter prediction is simpler and more effective\n",
    "- âœ… No need for complex RL (sparse rewards, huge state space)\n",
    "- âœ… Traditional HMM got 27% - we can beat it with neural nets!\n",
    "\n",
    "## Architecture:\n",
    "- **Input:** Masked word + guessed letters + word length (863 dims)\n",
    "- **Hidden:** 3 layers (512 â†’ 256 â†’ 128)\n",
    "- **Output:** 26 letter probabilities\n",
    "- **Training:** 10 epochs with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from hangman_env import HangmanEnv\n",
    "from utils import calculate_final_score\n",
    "import random\n",
    "\n",
    "# Seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ Device: {device}\")\n",
    "print(f\"âœ“ PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a993c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('../Data/corpus_cleaned.txt', 'r') as f:\n",
    "    corpus_words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "with open('../Data/test_cleaned.txt', 'r') as f:\n",
    "    test_words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"âœ“ Training words: {len(corpus_words):,}\")\n",
    "print(f\"âœ“ Test words: {len(test_words):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNeuralHMM(nn.Module):\n",
    "    \"\"\"Deeper, better neural network for letter prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_word_len=30):\n",
    "        super(ImprovedNeuralHMM, self).__init__()\n",
    "        self.max_word_len = max_word_len\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.alphabet)}\n",
    "        self.char_to_idx['_'] = 26\n",
    "        \n",
    "        # Input size: word (30*27) + guessed (26) + length (1) = 863\n",
    "        input_size = max_word_len * 27 + 26 + 1\n",
    "        \n",
    "        # Deeper network\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, 26)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def encode_state(self, masked_word, guessed_letters, word_length):\n",
    "        \"\"\"Encode game state.\"\"\"\n",
    "        # One-hot word\n",
    "        word_enc = np.zeros(self.max_word_len * 27)\n",
    "        for i, char in enumerate(masked_word[:self.max_word_len]):\n",
    "            idx = self.char_to_idx.get(char, 26) if char else 26\n",
    "            word_enc[i * 27 + idx] = 1.0\n",
    "        \n",
    "        # Binary guessed\n",
    "        guessed_enc = np.zeros(26)\n",
    "        for char in guessed_letters:\n",
    "            if char in self.char_to_idx and self.char_to_idx[char] < 26:\n",
    "                guessed_enc[self.char_to_idx[char]] = 1.0\n",
    "        \n",
    "        # Normalized length\n",
    "        length_enc = np.array([word_length / self.max_word_len])\n",
    "        \n",
    "        return np.concatenate([word_enc, guessed_enc, length_enc])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def predict_probs(self, masked_word, guessed_letters, word_length):\n",
    "        \"\"\"Predict letter probabilities.\"\"\"\n",
    "        state = self.encode_state(masked_word, guessed_letters, word_length)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(state_tensor)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Mask guessed\n",
    "        for char in guessed_letters:\n",
    "            if char in self.char_to_idx and self.char_to_idx[char] < 26:\n",
    "                probs[self.char_to_idx[char]] = 0.0\n",
    "        \n",
    "        # Normalize\n",
    "        if probs.sum() > 0:\n",
    "            probs = probs / probs.sum()\n",
    "        \n",
    "        return {self.alphabet[i]: probs[i] for i in range(26)}\n",
    "\n",
    "print(\"âœ“ ImprovedNeuralHMM defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(words, samples_per_word=3):\n",
    "    \"\"\"Generate training samples with data augmentation.\"\"\"\n",
    "    training_samples = []\n",
    "    \n",
    "    for word in tqdm(words, desc=\"Generating training data\"):\n",
    "        # Multiple samples per word with different masking strategies\n",
    "        for _ in range(samples_per_word):\n",
    "            # Random progression through word\n",
    "            letters = list(word)\n",
    "            random.shuffle(letters)\n",
    "            \n",
    "            guessed = set()\n",
    "            remaining = set(word)\n",
    "            \n",
    "            for letter in letters:\n",
    "                if letter in remaining:\n",
    "                    # Current masked state\n",
    "                    masked = [c if c in guessed else None for c in word]\n",
    "                    \n",
    "                    training_samples.append({\n",
    "                        'masked': masked,\n",
    "                        'guessed': guessed.copy(),\n",
    "                        'length': len(word),\n",
    "                        'target': letter\n",
    "                    })\n",
    "                    \n",
    "                    guessed.add(letter)\n",
    "                    remaining.discard(letter)\n",
    "                    \n",
    "                    # Stop after a few guesses to focus on early game\n",
    "                    if len(guessed) >= 4:\n",
    "                        break\n",
    "    \n",
    "    random.shuffle(training_samples)\n",
    "    return training_samples\n",
    "\n",
    "print(\"âœ“ Data generation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4520fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, training_samples, epochs=10, batch_size=256):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    print(f\"\\nTraining for {epochs} epochs on {len(training_samples):,} samples...\\n\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in tqdm(range(0, len(training_samples), batch_size), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch = training_samples[i:i+batch_size]\n",
    "            \n",
    "            # Prepare batch\n",
    "            states = []\n",
    "            targets = []\n",
    "            \n",
    "            for sample in batch:\n",
    "                state = model.encode_state(sample['masked'], sample['guessed'], sample['length'])\n",
    "                target_idx = model.char_to_idx[sample['target']]\n",
    "                states.append(state)\n",
    "                targets.append(target_idx)\n",
    "            \n",
    "            states = torch.FloatTensor(np.array(states)).to(device)\n",
    "            targets = torch.LongTensor(targets).to(device)\n",
    "            \n",
    "            # Forward + backward\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(states)\n",
    "            loss = criterion(logits, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / (len(training_samples) / batch_size)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print(f\"  Avg Loss: {avg_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    print(\"\\nâœ“ Training complete!\")\n",
    "\n",
    "print(\"âœ“ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53bd9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data (3 samples per word)\n",
    "training_samples = generate_training_data(corpus_words, samples_per_word=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f15a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model = ImprovedNeuralHMM().to(device)\n",
    "train_model(model, training_samples, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a08115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_words):\n",
    "    \"\"\"Evaluate on test set.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for word in tqdm(test_words, desc=\"Evaluating\"):\n",
    "        env = HangmanEnv(word, max_lives=6)\n",
    "        env.reset()\n",
    "        \n",
    "        while not env.done:\n",
    "            masked = env.get_masked_word_list()\n",
    "            probs = model.predict_probs(masked, env.guessed_letters, len(word))\n",
    "            \n",
    "            available = {k: v for k, v in probs.items() if k not in env.guessed_letters}\n",
    "            if available:\n",
    "                action = max(available, key=available.get)\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            env.step(action)\n",
    "        \n",
    "        stats = env.get_stats()\n",
    "        results.append({\n",
    "            'won': env.won,\n",
    "            'wrong': stats['wrong_count'],\n",
    "            'repeated': stats['repeated_count']\n",
    "        })\n",
    "    \n",
    "    wins = sum(1 for r in results if r['won'])\n",
    "    rate = wins / len(results)\n",
    "    wrong = sum(r['wrong'] for r in results)\n",
    "    repeated = sum(r['repeated'] for r in results)\n",
    "    score = calculate_final_score(rate, wrong, repeated, len(results))\n",
    "    \n",
    "    return rate, score, wrong, repeated\n",
    "\n",
    "print(\"âœ“ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a85817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on full test set\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING ON 2000 TEST WORDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rate, score, wrong, repeated = evaluate_model(model, test_words)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Win rate: {rate*100:.2f}%\")\n",
    "print(f\"Score: {score:.2f}\")\n",
    "print(f\"Wrong guesses: {wrong:,}\")\n",
    "print(f\"Repeated: {repeated}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Traditional HMM (n-grams):  27.05%\")\n",
    "print(f\"Neural HMM (PyTorch):       {rate*100:.2f}%\")\n",
    "print(f\"\\nðŸŽ¯ Improvement: {(rate*100 - 27.05):+.2f} pp\")\n",
    "\n",
    "if rate >= 0.30:\n",
    "    print(\"\\nâœ… SUCCESS: Beat traditional HMM with neural network!\")\n",
    "else:\n",
    "    print(\"\\nðŸ“ˆ Close! May need more training or architecture tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa9d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), '../models/neural_hmm_simple.pth')\n",
    "print(\"\\nâœ“ Model saved to models/neural_hmm_simple.pth\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
