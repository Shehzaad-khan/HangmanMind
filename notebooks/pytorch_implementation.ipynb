{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5370e470",
   "metadata": {},
   "source": [
    "# PyTorch Implementation: Improved Neural HMM\n",
    "\n",
    "**Simplified PyTorch solution - Neural HMM only (no DQN)**\n",
    "\n",
    "## Why No DQN?\n",
    "- ‚ùå Dataset too small (50K words) for deep RL\n",
    "- ‚ùå State space too large (sparse rewards)\n",
    "- ‚ùå Overfitting risk with DQN\n",
    "- ‚úÖ **Better approach:** Improved Neural HMM with more training\n",
    "\n",
    "## Components:\n",
    "1. **Neural HMM** - Deeper network, more epochs, better architecture\n",
    "2. **Direct prediction** - No RL agent needed\n",
    "\n",
    "## Previous Results:\n",
    "- Traditional HMM (n-grams): 27.05%\n",
    "- **Target with Neural HMM: 30%+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b961154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Using device: cpu\n",
      "‚úì PyTorch version: 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter, defaultdict, deque\n",
    "from tqdm import tqdm\n",
    "from hangman_env import HangmanEnv\n",
    "from utils import calculate_final_score\n",
    "import random\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì Using device: {device}\")\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f03e319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 49302 training words\n",
      "‚úì Loaded 2000 test words\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "with open('../Data/corpus_cleaned.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus_words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "with open('../Data/test_cleaned.txt', 'r', encoding='utf-8') as f:\n",
    "    test_words = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"‚úì Loaded {len(corpus_words)} training words\")\n",
    "print(f\"‚úì Loaded {len(test_words)} test words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a41271",
   "metadata": {},
   "source": [
    "## Part 1: Neural HMM\n",
    "\n",
    "Instead of counting n-grams, we use a neural network to learn letter patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60589be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì NeuralHMM class defined\n"
     ]
    }
   ],
   "source": [
    "class NeuralHMM(nn.Module):\n",
    "    \"\"\"Neural network that predicts letter probabilities given game state.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_word_len=30, hidden_size=256):\n",
    "        super(NeuralHMM, self).__init__()\n",
    "        self.max_word_len = max_word_len\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.alphabet)}\n",
    "        self.char_to_idx['_'] = 26  # Mask token\n",
    "        \n",
    "        # Input: masked word (one-hot) + guessed letters (binary) + position embeddings\n",
    "        # Word embedding: max_word_len * 27 (26 letters + mask)\n",
    "        # Guessed: 26 (binary vector)\n",
    "        # Word length: 1\n",
    "        input_size = max_word_len * 27 + 26 + 1\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 26)  # Output: probability for each letter\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def encode_state(self, masked_word, guessed_letters, word_length):\n",
    "        \"\"\"Encode game state as feature vector.\"\"\"\n",
    "        # One-hot encode masked word\n",
    "        word_encoding = np.zeros(self.max_word_len * 27)\n",
    "        for i, char in enumerate(masked_word[:self.max_word_len]):\n",
    "            if char is None:\n",
    "                idx = self.char_to_idx['_']\n",
    "            else:\n",
    "                idx = self.char_to_idx[char]\n",
    "            word_encoding[i * 27 + idx] = 1.0\n",
    "        \n",
    "        # Binary encode guessed letters\n",
    "        guessed_encoding = np.zeros(26)\n",
    "        for char in guessed_letters:\n",
    "            if char in self.char_to_idx and self.char_to_idx[char] < 26:\n",
    "                guessed_encoding[self.char_to_idx[char]] = 1.0\n",
    "        \n",
    "        # Normalize word length\n",
    "        length_encoding = np.array([word_length / self.max_word_len])\n",
    "        \n",
    "        return np.concatenate([word_encoding, guessed_encoding, length_encoding])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # Logits\n",
    "        return x\n",
    "    \n",
    "    def predict_probs(self, masked_word, guessed_letters, word_length):\n",
    "        \"\"\"Predict letter probabilities.\"\"\"\n",
    "        state = self.encode_state(masked_word, guessed_letters, word_length)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(state_tensor)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Mask already guessed letters\n",
    "        for char in guessed_letters:\n",
    "            if char in self.char_to_idx and self.char_to_idx[char] < 26:\n",
    "                probs[self.char_to_idx[char]] = 0.0\n",
    "        \n",
    "        # Renormalize\n",
    "        if probs.sum() > 0:\n",
    "            probs = probs / probs.sum()\n",
    "        \n",
    "        return {self.alphabet[i]: probs[i] for i in range(26)}\n",
    "\n",
    "print(\"‚úì NeuralHMM class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99d806",
   "metadata": {},
   "source": [
    "## Part 2: DQN Agent\n",
    "\n",
    "Deep Q-Network that uses Neural HMM guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced9ab6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì DQN and DQNAgent classes defined\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network for action selection.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size=26, hidden_size=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with experience replay and target network.\"\"\"\n",
    "    \n",
    "    def __init__(self, neural_hmm, state_size=863, action_size=26):\n",
    "        self.neural_hmm = neural_hmm\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Q-networks\n",
    "        self.policy_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.gamma = 0.95\n",
    "        self.batch_size = 64\n",
    "        self.target_update = 10\n",
    "        self.steps = 0\n",
    "    \n",
    "    def get_state_vector(self, env):\n",
    "        \"\"\"Convert environment state to feature vector (same as Neural HMM).\"\"\"\n",
    "        masked_word = env.get_masked_word_list()\n",
    "        return self.neural_hmm.encode_state(masked_word, env.guessed_letters, len(env.word))\n",
    "    \n",
    "    def select_action(self, env, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy with HMM guidance.\"\"\"\n",
    "        available_actions = [i for i, c in enumerate(self.alphabet) if c not in env.guessed_letters]\n",
    "        \n",
    "        if not available_actions:\n",
    "            return None\n",
    "        \n",
    "        # Epsilon-greedy exploration\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Explore: Use HMM probabilities\n",
    "            masked = env.get_masked_word_list()\n",
    "            hmm_probs = self.neural_hmm.predict_probs(masked, env.guessed_letters, len(env.word))\n",
    "            probs = np.array([hmm_probs[c] for c in self.alphabet])\n",
    "            probs = probs[available_actions]\n",
    "            \n",
    "            if probs.sum() > 0:\n",
    "                probs = probs / probs.sum()\n",
    "                action = np.random.choice(available_actions, p=probs)\n",
    "            else:\n",
    "                action = random.choice(available_actions)\n",
    "        else:\n",
    "            # Exploit: Use Q-values + HMM guidance\n",
    "            state = self.get_state_vector(env)\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state_tensor).cpu().numpy()[0]\n",
    "            \n",
    "            # Add HMM guidance\n",
    "            masked = env.get_masked_word_list()\n",
    "            hmm_probs = self.neural_hmm.predict_probs(masked, env.guessed_letters, len(env.word))\n",
    "            for i, c in enumerate(self.alphabet):\n",
    "                q_values[i] += hmm_probs[c] * 2.0  # HMM weight\n",
    "            \n",
    "            # Select best available action\n",
    "            action = max(available_actions, key=lambda a: q_values[a])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train on batch of experiences.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(dones).to(device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network.\"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"‚úì DQN and DQNAgent classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60999243",
   "metadata": {},
   "source": [
    "## Part 3: Train Neural HMM\n",
    "\n",
    "Supervised training on corpus words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65a3ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural HMM for 3 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49302/49302 [02:51<00:00, 287.25it/s]\n",
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49302/49302 [02:51<00:00, 287.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 - Avg Loss: 2.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49302/49302 [02:50<00:00, 289.94it/s]\n",
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49302/49302 [02:50<00:00, 289.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 - Avg Loss: 2.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49302/49302 [02:49<00:00, 290.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 - Avg Loss: 2.9734\n",
      "‚úì Neural HMM training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_neural_hmm(model, words, epochs=10, batch_size=128):\n",
    "    \"\"\"Train Neural HMM using supervised learning.\"\"\"\n",
    "    print(f\"Training Neural HMM for {epochs} epochs...\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Shuffle words\n",
    "        random.shuffle(words)\n",
    "        \n",
    "        for word in tqdm(words, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            # Simulate game states by progressively revealing letters\n",
    "            guessed = set()\n",
    "            remaining = set(word)\n",
    "            \n",
    "            while remaining:\n",
    "                # Create masked word\n",
    "                masked = [c if c in guessed else None for c in word]\n",
    "                \n",
    "                # Pick a random unguessed letter from the word\n",
    "                target_letter = random.choice(list(remaining))\n",
    "                target_idx = model.char_to_idx[target_letter]\n",
    "                \n",
    "                # Encode state\n",
    "                state = model.encode_state(masked, guessed, len(word))\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                target_tensor = torch.LongTensor([target_idx]).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = model(state_tensor)\n",
    "                loss = criterion(logits, target_tensor)\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Update guessed and remaining\n",
    "                guessed.add(target_letter)\n",
    "                remaining.discard(target_letter)\n",
    "                \n",
    "                # Batch limit to avoid too many updates per word\n",
    "                if len(guessed) >= 3:  # Train on first few guesses\n",
    "                    break\n",
    "        \n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        print(f\"  Epoch {epoch+1} - Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"‚úì Neural HMM training complete\")\n",
    "\n",
    "# Create and train Neural HMM\n",
    "neural_hmm = NeuralHMM().to(device)\n",
    "train_neural_hmm(neural_hmm, corpus_words, epochs=3)  # Start with 3 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c609dbe",
   "metadata": {},
   "source": [
    "## Part 4: Test Neural HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b55bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Neural HMM on 500 test words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Neural HMM: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 1131.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NEURAL HMM Performance:\n",
      "======================================================================\n",
      "Win rate: 13.80%\n",
      "Score: -14291.00\n",
      "Wrong guesses: 2872\n",
      "Repeated guesses: 0\n",
      "\n",
      "Comparison to traditional HMM: 27.05%\n",
      "Difference: -13.25 pp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_neural_hmm(model, test_words, max_test=500):\n",
    "    \"\"\"Test Neural HMM performance.\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for word in tqdm(test_words[:max_test], desc=\"Testing Neural HMM\"):\n",
    "        env = HangmanEnv(word, max_lives=6)\n",
    "        env.reset()\n",
    "        \n",
    "        while not env.done:\n",
    "            masked = env.get_masked_word_list()\n",
    "            probs = model.predict_probs(masked, env.guessed_letters, len(word))\n",
    "            \n",
    "            # Pick best available letter\n",
    "            available = {k: v for k, v in probs.items() if k not in env.guessed_letters}\n",
    "            if available:\n",
    "                action = max(available, key=available.get)\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "            env.step(action)\n",
    "        \n",
    "        stats = env.get_stats()\n",
    "        results.append({'won': env.won, 'wrong': stats['wrong_count'], 'repeated': stats['repeated_count']})\n",
    "    \n",
    "    wins = sum(1 for r in results if r['won'])\n",
    "    rate = wins / len(results)\n",
    "    wrong = sum(r['wrong'] for r in results)\n",
    "    repeated = sum(r['repeated'] for r in results)\n",
    "    score = calculate_final_score(rate, wrong, repeated, len(results))\n",
    "    \n",
    "    return rate, score, wrong, repeated\n",
    "\n",
    "print(\"Testing Neural HMM on 500 test words...\")\n",
    "hmm_rate, hmm_score, hmm_wrong, hmm_repeated = test_neural_hmm(neural_hmm, test_words, 500)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEURAL HMM Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Win rate: {hmm_rate*100:.2f}%\")\n",
    "print(f\"Score: {hmm_score:.2f}\")\n",
    "print(f\"Wrong guesses: {hmm_wrong}\")\n",
    "print(f\"Repeated guesses: {hmm_repeated}\")\n",
    "print(f\"\\nComparison to traditional HMM: 27.05%\")\n",
    "print(f\"Difference: {(hmm_rate*100 - 27.05):.2f} pp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ddbd3",
   "metadata": {},
   "source": [
    "## Part 5: Train DQN Agent\n",
    "\n",
    "Train with experience replay and target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c57b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DQN for 10000 episodes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DQN:   0%|          | 6/10000 [00:00<00:25, 390.33it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x837 and 863x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Create and train DQN agent\u001b[39;00m\n\u001b[1;32m     66\u001b[0m dqn_agent \u001b[38;5;241m=\u001b[39m DQNAgent(neural_hmm)\n\u001b[0;32m---> 67\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mtrain_dqn_agent\u001b[0;34m(agent, train_words, episodes)\u001b[0m\n\u001b[1;32m     15\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Select action\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     action_idx \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 74\u001b[0m, in \u001b[0;36mDQNAgent.select_action\u001b[0;34m(self, env, training)\u001b[0m\n\u001b[1;32m     71\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 74\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Add HMM guidance\u001b[39;00m\n\u001b[1;32m     77\u001b[0m masked \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_masked_word_list()\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x837 and 863x128)"
     ]
    }
   ],
   "source": [
    "def train_dqn_agent(agent, train_words, episodes=10000):\n",
    "    \"\"\"Train DQN agent.\"\"\"\n",
    "    print(f\"Training DQN for {episodes} episodes...\\n\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    recent_wins = deque(maxlen=100)\n",
    "    losses = []\n",
    "    \n",
    "    for episode in tqdm(range(episodes), desc=\"Training DQN\"):\n",
    "        # Sample random word\n",
    "        word = random.choice(train_words)\n",
    "        env = HangmanEnv(word, max_lives=6)\n",
    "        state = agent.get_state_vector(env)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not env.done:\n",
    "            # Select action\n",
    "            action_idx = agent.select_action(env, training=True)\n",
    "            if action_idx is None:\n",
    "                break\n",
    "            \n",
    "            action = agent.alphabet[action_idx]\n",
    "            \n",
    "            # Take action\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            next_state = agent.get_state_vector(env)\n",
    "            \n",
    "            # Store experience\n",
    "            agent.remember(state, action_idx, reward, next_state, done)\n",
    "            \n",
    "            # Train\n",
    "            loss = agent.replay()\n",
    "            if loss > 0:\n",
    "                losses.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            agent.steps += 1\n",
    "        \n",
    "        # Update target network\n",
    "        if episode % agent.target_update == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Track metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        recent_wins.append(1 if env.won else 0)\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Log progress\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            win_rate = np.mean(recent_wins) if len(recent_wins) > 0 else 0\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_loss = np.mean(losses[-100:]) if len(losses) > 100 else 0\n",
    "            \n",
    "            print(f\"\\nEpisode {episode + 1}/{episodes}:\")\n",
    "            print(f\"  Win rate (last 100): {win_rate*100:.2f}%\")\n",
    "            print(f\"  Avg reward: {avg_reward:.2f}\")\n",
    "            print(f\"  Avg loss: {avg_loss:.4f}\")\n",
    "            print(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "            print(f\"  Memory size: {len(agent.memory)}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "# Create and train DQN agent\n",
    "dqn_agent = DQNAgent(neural_hmm)\n",
    "rewards = train_dqn_agent(dqn_agent, corpus_words, episodes=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ebaa9",
   "metadata": {},
   "source": [
    "## Part 6: Evaluate DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf0ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dqn_agent(agent, test_words):\n",
    "    \"\"\"Evaluate DQN agent on test set.\"\"\"\n",
    "    agent.policy_net.eval()\n",
    "    results = []\n",
    "    \n",
    "    for word in tqdm(test_words, desc=\"Evaluating DQN\"):\n",
    "        env = HangmanEnv(word, max_lives=6)\n",
    "        env.reset()\n",
    "        \n",
    "        while not env.done:\n",
    "            action_idx = agent.select_action(env, training=False)\n",
    "            if action_idx is None:\n",
    "                break\n",
    "            \n",
    "            action = agent.alphabet[action_idx]\n",
    "            env.step(action)\n",
    "        \n",
    "        stats = env.get_stats()\n",
    "        results.append({\n",
    "            'won': env.won,\n",
    "            'wrong': stats['wrong_count'],\n",
    "            'repeated': stats['repeated_count']\n",
    "        })\n",
    "    \n",
    "    wins = sum(1 for r in results if r['won'])\n",
    "    rate = wins / len(results)\n",
    "    wrong = sum(r['wrong'] for r in results)\n",
    "    repeated = sum(r['repeated'] for r in results)\n",
    "    score = calculate_final_score(rate, wrong, repeated, len(results))\n",
    "    \n",
    "    return rate, score, wrong, repeated\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING ON FULL TEST SET (2000 words)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rate, score, wrong, repeated = evaluate_dqn_agent(dqn_agent, test_words)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Win rate: {rate*100:.2f}%\")\n",
    "print(f\"Final score: {score:.2f}\")\n",
    "print(f\"Wrong guesses: {wrong:,}\")\n",
    "print(f\"Repeated guesses: {repeated}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON TO PREVIOUS APPROACHES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Traditional HMM + Tabular Q-learning: 27.05%\")\n",
    "print(f\"Neural HMM alone:                     {hmm_rate*100:.2f}%\")\n",
    "print(f\"PyTorch DQN + Neural HMM:             {rate*100:.2f}%\")\n",
    "print(f\"\\nüéØ Improvement: {(rate*100 - 27.05):+.2f} percentage points\")\n",
    "\n",
    "if rate >= 0.35:\n",
    "    print(\"\\nüéâ TARGET ACHIEVED: 35%+ with PyTorch!\")\n",
    "elif rate >= 0.30:\n",
    "    print(\"\\n‚úÖ GOOD PROGRESS: 30%+ win rate\")\n",
    "else:\n",
    "    print(\"\\nüìà Need more training or hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889bd414",
   "metadata": {},
   "source": [
    "## Part 7: Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedcba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Neural HMM\n",
    "torch.save(neural_hmm.state_dict(), '../models/neural_hmm.pth')\n",
    "print(\"‚úì Saved Neural HMM to models/neural_hmm.pth\")\n",
    "\n",
    "# Save DQN\n",
    "torch.save({\n",
    "    'policy_net': dqn_agent.policy_net.state_dict(),\n",
    "    'target_net': dqn_agent.target_net.state_dict(),\n",
    "    'optimizer': dqn_agent.optimizer.state_dict(),\n",
    "    'epsilon': dqn_agent.epsilon\n",
    "}, '../models/dqn_agent.pth')\n",
    "print(\"‚úì Saved DQN Agent to models/dqn_agent.pth\")\n",
    "\n",
    "print(\"\\n‚úÖ All PyTorch models saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
