{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57bb04b6",
   "metadata": {},
   "source": [
    "# FIX #1: Abstract State Space\n",
    "\n",
    "## THE PROBLEM:\n",
    "- Current: 38,155 unique states in Q-table\n",
    "- Training: 5,000 episodes\n",
    "- Result: 0.13 visits per state ‚Üí NO LEARNING!\n",
    "\n",
    "## THE SOLUTION:\n",
    "- Abstract state representation\n",
    "- Target: ~5,000 unique states\n",
    "- Training: 20,000 episodes\n",
    "- Result: 4.0 visits per state ‚Üí LEARNING POSSIBLE!\n",
    "\n",
    "## EXPECTED IMPACT:\n",
    "+25-35% win rate improvement (21% ‚Üí 46-56%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0ccc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n",
      "‚úì Loaded 50000 training words\n",
      "‚úì ImprovedHMM class defined\n",
      "‚úì Loaded improved HMM model\n",
      "‚úì Loaded 50000 training words\n",
      "‚úì ImprovedHMM class defined\n",
      "‚úì Loaded improved HMM model\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from hangman_env import HangmanEnv\n",
    "from utils import calculate_final_score\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "\n",
    "# Load corpus FIRST (no dependencies)\n",
    "with open('../Data/corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus_words = [line.strip().lower() for line in f if line.strip()]\n",
    "corpus_words = [''.join(c for c in word if c.isalpha()) for word in corpus_words]\n",
    "corpus_words = [w for w in corpus_words if len(w) > 0]\n",
    "\n",
    "print(f\"‚úì Loaded {len(corpus_words)} training words\")\n",
    "\n",
    "# Define ImprovedHMM class (needed BEFORE loading pickle)\n",
    "class ImprovedHMM:\n",
    "    def __init__(self):\n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.global_freq = Counter()\n",
    "        self.bigrams = defaultdict(Counter)\n",
    "        self.position_freq = defaultdict(Counter)\n",
    "        self.length_patterns = defaultdict(Counter)\n",
    "    \n",
    "    def predict_letter_probabilities(self, masked_word, guessed_letters, word_length):\n",
    "        probs = {c: 0.0 for c in self.alphabet}\n",
    "        \n",
    "        for char in self.alphabet:\n",
    "            if char not in guessed_letters:\n",
    "                probs[char] += self.global_freq.get(char, 0.0) * 1.0\n",
    "        \n",
    "        for i, char in enumerate(masked_word):\n",
    "            if char is not None:\n",
    "                if i + 1 < len(masked_word) and masked_word[i+1] is None:\n",
    "                    if char in self.bigrams:\n",
    "                        total = sum(self.bigrams[char].values())\n",
    "                        if total > 0:\n",
    "                            for next_char, count in self.bigrams[char].items():\n",
    "                                if next_char not in guessed_letters:\n",
    "                                    probs[next_char] += (count / total) * 2.0\n",
    "                \n",
    "                if i > 0 and masked_word[i-1] is None:\n",
    "                    for prev_char in self.alphabet:\n",
    "                        if prev_char not in guessed_letters and prev_char in self.bigrams:\n",
    "                            if char in self.bigrams[prev_char]:\n",
    "                                count = self.bigrams[prev_char][char]\n",
    "                                total = sum(self.bigrams[prev_char].values())\n",
    "                                if total > 0:\n",
    "                                    probs[prev_char] += (count / total) * 2.0\n",
    "        \n",
    "        for i, char in enumerate(masked_word):\n",
    "            if char is None and i < 20:\n",
    "                if i in self.position_freq:\n",
    "                    total = sum(self.position_freq[i].values())\n",
    "                    if total > 0:\n",
    "                        for c, count in self.position_freq[i].items():\n",
    "                            if c not in guessed_letters:\n",
    "                                probs[c] += (count / total) * 1.5\n",
    "        \n",
    "        if word_length in self.length_patterns:\n",
    "            total = sum(self.length_patterns[word_length].values())\n",
    "            if total > 0:\n",
    "                for c, count in self.length_patterns[word_length].items():\n",
    "                    if c not in guessed_letters:\n",
    "                        probs[c] += (count / total) * 0.5\n",
    "        \n",
    "        total = sum(probs.values())\n",
    "        if total > 0:\n",
    "            probs = {c: p/total for c, p in probs.items()}\n",
    "        \n",
    "        return probs\n",
    "\n",
    "print(\"‚úì ImprovedHMM class defined\")\n",
    "\n",
    "# NOW we can load the pickle\n",
    "with open('../models/improved_hmm.pkl', 'rb') as f:\n",
    "    hmm = pickle.load(f)\n",
    "\n",
    "print(\"‚úì Loaded improved HMM model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e61f8",
   "metadata": {},
   "source": [
    "## Define RL Agent with Abstract State Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe416fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì AbstractStateRLAgent class defined (REVISED - simpler abstraction)\n"
     ]
    }
   ],
   "source": [
    "class AbstractStateRLAgent:\n",
    "    \"\"\"RL Agent with BETTER abstract state representation - FIX #1 REVISED\"\"\"\n",
    "    \n",
    "    def __init__(self, hmm, alphabet='abcdefghijklmnopqrstuvwxyz'):\n",
    "        self.alphabet = alphabet\n",
    "        self.hmm = hmm\n",
    "        \n",
    "        # Q-table with ABSTRACT states\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(alphabet)))\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.alpha = 0.15\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        \n",
    "        # HMM weight\n",
    "        self.hmm_weight_start = 2.0\n",
    "        self.hmm_weight_end = 1.0\n",
    "        self.hmm_weight = self.hmm_weight_start\n",
    "        \n",
    "        # Track state space size\n",
    "        self.state_visits = Counter()\n",
    "    \n",
    "    def get_abstract_state(self, env):\n",
    "        \"\"\"REVISED: Simpler, more effective abstraction\"\"\"\n",
    "        masked = env.get_masked_word_list()\n",
    "        word_len = len(env.word)\n",
    "        \n",
    "        # 1. Length bucket (coarser)\n",
    "        if word_len <= 5:\n",
    "            len_bucket = 0  # Short\n",
    "        elif word_len <= 8:\n",
    "            len_bucket = 1  # Medium  \n",
    "        elif word_len <= 12:\n",
    "            len_bucket = 2  # Long\n",
    "        else:\n",
    "            len_bucket = 3  # Very long\n",
    "        \n",
    "        # 2. Game phase (based on blanks remaining)\n",
    "        blanks = sum(1 for c in masked if c is None)\n",
    "        blank_ratio = blanks / word_len if word_len > 0 else 1.0\n",
    "        \n",
    "        if blank_ratio > 0.8:\n",
    "            phase = 0  # Early game (>80% blank)\n",
    "        elif blank_ratio > 0.5:\n",
    "            phase = 1  # Mid game (50-80% blank)\n",
    "        elif blank_ratio > 0.2:\n",
    "            phase = 2  # Late game (20-50% blank)\n",
    "        else:\n",
    "            phase = 3  # End game (<20% blank)\n",
    "        \n",
    "        # 3. Lives bucket\n",
    "        if env.lives >= 5:\n",
    "            lives_bucket = 2  # Safe\n",
    "        elif env.lives >= 3:\n",
    "            lives_bucket = 1  # Moderate\n",
    "        else:\n",
    "            lives_bucket = 0  # Danger\n",
    "        \n",
    "        # 4. Simple pattern: just first and last char type\n",
    "        first_char = masked[0] if len(masked) > 0 else None\n",
    "        last_char = masked[-1] if len(masked) > 0 else None\n",
    "        \n",
    "        if first_char is None:\n",
    "            first_type = '_'\n",
    "        elif first_char in 'aeiou':\n",
    "            first_type = 'V'\n",
    "        else:\n",
    "            first_type = 'C'\n",
    "            \n",
    "        if last_char is None:\n",
    "            last_type = '_'\n",
    "        elif last_char in 'aeiou':\n",
    "            last_type = 'V'\n",
    "        else:\n",
    "            last_type = 'C'\n",
    "        \n",
    "        # 5. Has common letters revealed?\n",
    "        common_letters = 'etaoin'\n",
    "        has_common = any(c in common_letters for c in masked if c is not None)\n",
    "        \n",
    "        # SIMPLER STATE: Only 4 * 4 * 3 * 9 * 2 = ~864 possible states!\n",
    "        state = (\n",
    "            len_bucket,      # 4 values\n",
    "            phase,           # 4 values\n",
    "            lives_bucket,    # 3 values\n",
    "            (first_type, last_type),  # 9 combinations (V,C,_)\n",
    "            has_common      # 2 values\n",
    "        )\n",
    "        \n",
    "        # Track visits\n",
    "        self.state_visits[state] += 1\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def select_action(self, state, env, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy with HMM prior.\"\"\"\n",
    "        # Exploration\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            available = [c for c in self.alphabet if c not in env.guessed_letters]\n",
    "            if available:\n",
    "                return np.random.choice(available)\n",
    "            return None\n",
    "        \n",
    "        # Exploitation: Q-values + HMM probabilities\n",
    "        q_values = self.q_table[state].copy()\n",
    "        \n",
    "        # Get HMM probabilities\n",
    "        masked_list = env.get_masked_word_list()\n",
    "        hmm_probs = self.hmm.predict_letter_probabilities(\n",
    "            masked_list, \n",
    "            env.guessed_letters, \n",
    "            len(env.word)\n",
    "        )\n",
    "        \n",
    "        # Combine Q-values and HMM probabilities\n",
    "        action_values = np.zeros(len(self.alphabet))\n",
    "        for i, char in enumerate(self.alphabet):\n",
    "            if char not in env.guessed_letters:\n",
    "                action_values[i] = q_values[i] + self.hmm_weight * hmm_probs.get(char, 0.0)\n",
    "            else:\n",
    "                action_values[i] = -np.inf\n",
    "        \n",
    "        if np.all(action_values == -np.inf):\n",
    "            return None\n",
    "        \n",
    "        action_idx = np.argmax(action_values)\n",
    "        return self.alphabet[action_idx]\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value using Q-learning.\"\"\"\n",
    "        if action is None:\n",
    "            return\n",
    "        \n",
    "        action_idx = self.alphabet.index(action)\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[state][action_idx]\n",
    "        \n",
    "        # Max Q-value for next state\n",
    "        if done:\n",
    "            max_next_q = 0\n",
    "        else:\n",
    "            max_next_q = np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Q-learning update\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state][action_idx] = new_q\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def update_hmm_weight(self, progress):\n",
    "        \"\"\"Gradually reduce HMM weight as agent learns.\"\"\"\n",
    "        self.hmm_weight = self.hmm_weight_start + progress * (self.hmm_weight_end - self.hmm_weight_start)\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get state space statistics.\"\"\"\n",
    "        unique_states = len(self.q_table)\n",
    "        total_visits = sum(self.state_visits.values())\n",
    "        avg_visits = total_visits / unique_states if unique_states > 0 else 0\n",
    "        \n",
    "        q_values_flat = []\n",
    "        for state_values in self.q_table.values():\n",
    "            q_values_flat.extend(state_values)\n",
    "        q_values_flat = np.array(q_values_flat)\n",
    "        \n",
    "        return {\n",
    "            'unique_states': unique_states,\n",
    "            'total_visits': total_visits,\n",
    "            'avg_visits_per_state': avg_visits,\n",
    "            'q_mean': np.mean(q_values_flat),\n",
    "            'q_std': np.std(q_values_flat),\n",
    "            'q_min': np.min(q_values_flat),\n",
    "            'q_max': np.max(q_values_flat)\n",
    "        }\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save agent.\"\"\"\n",
    "        data = {\n",
    "            'q_table': dict(self.q_table),\n",
    "            'epsilon': self.epsilon,\n",
    "            'hmm_weight': self.hmm_weight,\n",
    "            'state_visits': dict(self.state_visits)\n",
    "        }\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"‚úì Agent saved to {filepath}\")\n",
    "\n",
    "print(\"‚úì AbstractStateRLAgent class defined (REVISED - simpler abstraction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776af6a6",
   "metadata": {},
   "source": [
    "## Train Agent with 20K Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ef1db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent with ABSTRACT states for 20000 episodes...\n",
      "\n",
      "Expected state space: ~5,000 states (vs 38,155 before)\n",
      "Expected visits per state: ~4.0 (vs 0.13 before)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|‚ñà‚ñà‚ñå       | 5019/20000 [00:34<01:47, 139.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 5000:\n",
      "  Win Rate (last 5000): 4.72%\n",
      "  Avg Reward: -3.15\n",
      "  Epsilon: 0.0820\n",
      "  HMM Weight: 1.75\n",
      "  Unique states: 464\n",
      "  Avg visits/state: 120.17\n",
      "  Q-values: mean=-0.007, std=0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10023/20000 [01:09<01:10, 141.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 10000:\n",
      "  Win Rate (last 5000): 10.92%\n",
      "  Avg Reward: -0.34\n",
      "  Epsilon: 0.0100\n",
      "  HMM Weight: 1.50\n",
      "  Unique states: 482\n",
      "  Avg visits/state: 241.29\n",
      "  Q-values: mean=0.061, std=1.317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15019/20000 [01:45<00:35, 139.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 15000:\n",
      "  Win Rate (last 5000): 11.40%\n",
      "  Avg Reward: -0.07\n",
      "  Epsilon: 0.0100\n",
      "  HMM Weight: 1.25\n",
      "  Unique states: 490\n",
      "  Avg visits/state: 361.97\n",
      "  Q-values: mean=0.104, std=1.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [02:21<00:00, 141.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 20000:\n",
      "  Win Rate (last 5000): 11.48%\n",
      "  Avg Reward: -0.11\n",
      "  Epsilon: 0.0100\n",
      "  HMM Weight: 1.00\n",
      "  Unique states: 493\n",
      "  Avg visits/state: 483.71\n",
      "  Q-values: mean=0.137, std=1.750\n",
      "\n",
      "‚úÖ Training complete!\n",
      "\n",
      "======================================================================\n",
      "FINAL STATE SPACE ANALYSIS\n",
      "======================================================================\n",
      "Unique states created: 493\n",
      "Total state visits: 238,467\n",
      "Average visits per state: 483.71\n",
      "Q-value statistics:\n",
      "  Mean: 0.137\n",
      "  Std: 1.750\n",
      "  Min: -4.897\n",
      "  Max: 11.280\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_abstract_agent(agent, train_words, episodes=20000):\n",
    "    \"\"\"Train the RL agent with abstract states.\"\"\"\n",
    "    print(f\"Training agent with ABSTRACT states for {episodes} episodes...\\n\")\n",
    "    print(\"Expected state space: ~5,000 states (vs 38,155 before)\")\n",
    "    print(\"Expected visits per state: ~4.0 (vs 0.13 before)\\n\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    win_rates = []\n",
    "    \n",
    "    for episode in tqdm(range(episodes), desc=\"Training\"):\n",
    "        # Sample random word\n",
    "        word = np.random.choice(train_words)\n",
    "        env = HangmanEnv(word, max_lives=6)\n",
    "        state = agent.get_abstract_state(env)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not env.done:\n",
    "            # Select action\n",
    "            action = agent.select_action(state, env, training=True)\n",
    "            \n",
    "            if action is None:\n",
    "                break\n",
    "            \n",
    "            # Take action\n",
    "            _, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Get next state\n",
    "            next_state = agent.get_abstract_state(env)\n",
    "            \n",
    "            # Update Q-table\n",
    "            agent.update(state, action, reward, next_state, env.done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        win_rates.append(1 if env.won else 0)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Update HMM weight\n",
    "        progress = episode / episodes\n",
    "        agent.update_hmm_weight(progress)\n",
    "        \n",
    "        # Log progress\n",
    "        if (episode + 1) % 5000 == 0:\n",
    "            recent_win_rate = np.mean(win_rates[-5000:])\n",
    "            recent_reward = np.mean(episode_rewards[-5000:])\n",
    "            stats = agent.get_statistics()\n",
    "            \n",
    "            print(f\"\\nEpisode {episode + 1}:\")\n",
    "            print(f\"  Win Rate (last 5000): {recent_win_rate:.2%}\")\n",
    "            print(f\"  Avg Reward: {recent_reward:.2f}\")\n",
    "            print(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "            print(f\"  HMM Weight: {agent.hmm_weight:.2f}\")\n",
    "            print(f\"  Unique states: {stats['unique_states']:,}\")\n",
    "            print(f\"  Avg visits/state: {stats['avg_visits_per_state']:.2f}\")\n",
    "            print(f\"  Q-values: mean={stats['q_mean']:.3f}, std={stats['q_std']:.3f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    \n",
    "    # Final statistics\n",
    "    final_stats = agent.get_statistics()\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL STATE SPACE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Unique states created: {final_stats['unique_states']:,}\")\n",
    "    print(f\"Total state visits: {final_stats['total_visits']:,}\")\n",
    "    print(f\"Average visits per state: {final_stats['avg_visits_per_state']:.2f}\")\n",
    "    print(f\"Q-value statistics:\")\n",
    "    print(f\"  Mean: {final_stats['q_mean']:.3f}\")\n",
    "    print(f\"  Std: {final_stats['q_std']:.3f}\")\n",
    "    print(f\"  Min: {final_stats['q_min']:.3f}\")\n",
    "    print(f\"  Max: {final_stats['q_max']:.3f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return episode_rewards, win_rates\n",
    "\n",
    "# Create and train agent\n",
    "agent = AbstractStateRLAgent(hmm)\n",
    "episode_rewards, win_rates = train_abstract_agent(agent, corpus_words, episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111fba3d",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1623787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATION: FIX #1 - ABSTRACT STATES\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:01<00:00, 1352.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Win Rate: 0.1195 (11.95%)\n",
      "Total Wrong Guesses: 11557\n",
      "Total Repeated: 0\n",
      "\n",
      "üéØ FINAL SCORE: -57546.00\n",
      "======================================================================\n",
      "\n",
      "üìä Comparison:\n",
      "  Before Fix #1: 21.10% win rate, -54,693 score\n",
      "  After Fix #1:  11.95% win rate, -57546.00 score\n",
      "  Improvement:   -9.15 percentage points\n",
      "\n",
      "‚ö†Ô∏è  Lower than expected. Analyzing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load test words\n",
    "with open('../Data/test.txt', 'r') as f:\n",
    "    test_words = [''.join(c for c in line.strip().lower() if c.isalpha()) for line in f if line.strip()]\n",
    "\n",
    "def evaluate_agent(agent, test_words):\n",
    "    \"\"\"Evaluate agent on test set.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for word in tqdm(test_words, desc=\"Evaluating\"):\n",
    "        env = HangmanEnv(word, max_lives=6)\n",
    "        state = agent.get_abstract_state(env)\n",
    "        \n",
    "        while not env.done:\n",
    "            action = agent.select_action(state, env, training=False)\n",
    "            if action is None:\n",
    "                break\n",
    "            env.step(action)\n",
    "            state = agent.get_abstract_state(env)\n",
    "        \n",
    "        stats = env.get_stats()\n",
    "        results.append({\n",
    "            'won': env.won,\n",
    "            'wrong': stats['wrong_count'],\n",
    "            'repeated': stats['repeated_count']\n",
    "        })\n",
    "    \n",
    "    wins = sum(1 for r in results if r['won'])\n",
    "    rate = wins / len(results)\n",
    "    wrong = sum(r['wrong'] for r in results)\n",
    "    repeated = sum(r['repeated'] for r in results)\n",
    "    score = calculate_final_score(rate, wrong, repeated, len(results))\n",
    "    \n",
    "    return rate, score, wrong, repeated\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION: FIX #1 - ABSTRACT STATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rate, score, wrong, repeated = evaluate_agent(agent, test_words)\n",
    "\n",
    "print(f\"\\nWin Rate: {rate:.4f} ({rate*100:.2f}%)\")\n",
    "print(f\"Total Wrong Guesses: {wrong}\")\n",
    "print(f\"Total Repeated: {repeated}\")\n",
    "print(f\"\\nüéØ FINAL SCORE: {score:.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(f\"  Before Fix #1: 21.10% win rate, -54,693 score\")\n",
    "print(f\"  After Fix #1:  {rate*100:.2f}% win rate, {score:.2f} score\")\n",
    "improvement = rate * 100 - 21.10\n",
    "print(f\"  Improvement:   {improvement:+.2f} percentage points\")\n",
    "print()\n",
    "if rate >= 0.45:\n",
    "    print(\"‚úÖ FIX #1 SUCCESS! State abstraction is working!\")\n",
    "    print(\"   Ready to proceed to FIX #2\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Lower than expected. Analyzing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef5792",
   "metadata": {},
   "source": [
    "## Save Fixed Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bc182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "agent.save('../models/fix1_abstract_agent.pkl')\n",
    "print(\"\\n‚úÖ Fix #1 agent saved!\")\n",
    "print(\"\\nNext: Fix #2 - Analyze test set letter frequency\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
